<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[USB笔记1_UsbMouse]]></title>
    <url>%2F2020%2F08%2F11%2FUSB%E7%AC%94%E8%AE%B01-UsbMouse%2F</url>
    <content type="text"><![CDATA[USB笔记1_UsbMouse本篇主要来自《圈圈教你玩usb》的UsbMouse实例 一、USB枚举过程1.1 Device设备端代码片段12345678910111213141516while(1) //死循环&#123; if(D12GetIntPin()==0) //如果有中断发生 &#123; D12WriteCommand(READ_INTERRUPT_REGISTER); //写读中断寄存器的命令 InterruptSource=D12ReadByte(); //读回第一字节的中断寄存器 if(InterruptSource&amp;0x80)UsbBusSuspend(); //总线挂起中断处理 if(InterruptSource&amp;0x40)UsbBusReset(); //总线复位中断处理 if(InterruptSource&amp;0x01)UsbEp0Out(); //端点0输出中断处理 if(InterruptSource&amp;0x02)UsbEp0In(); //端点0输入中断处理 if(InterruptSource&amp;0x04)UsbEp1Out(); //端点1输出中断处理 if(InterruptSource&amp;0x08)UsbEp1In(); //端点1输入中断处理 if(InterruptSource&amp;0x10)UsbEp2Out(); //端点2输出中断处理 if(InterruptSource&amp;0x20)UsbEp2In(); //端点2输入中断处理 &#125;&#125; 1.2 Device设备端日志1.2.1 SETUP数据包，获取设备描述符123456789USB端点0输出中断。 // host端带返回值的请求，device需要知道输出什么，所以先读8字节读端点0缓冲区8字节。0x80 0x06 0x00 0x01 0x00 0x00 0x40 0x00 USB标准输入请求：获取描述符——设备描述符。 // 设备描述符有18个字节写端点0缓冲区16字节。 // PDIUSBD12的端点0大小的16字节0x12 0x01 0x10 0x01 0x00 0x00 0x00 0x10 0x88 0x88 0x01 0x00 0x00 0x01 0x01 0x02 USB端点0输入中断。写端点0缓冲区2字节。 // 发送剩余的2个字节0x03 0x01 日志显示已经成功接收到主机发送过来的8字节数据。在第一次接收到数据后，会停顿一段时间。这段时间主机一直在请求输入。但是目前还没有返回数据，所以D12一直在回答NAK，即没有数据准备好。结果USB主机经过一段时间的等待之后，终于不耐烦了，发送了一次总线复位，然后又重新输出这8个字节的数据，然后又是等待输入数据。尝试几次后主机只好无奈的放弃了。这是改USB端口上不再有数据活动，从而D12进入了挂起状态。同时在计算机端弹出无法识别的USB设备对话框。 主机端内核日志： [ 9536.933549] usb 2-1: new full-speed USB device number 16 using ci_hdrc [ 9542.053622] usb 2-1: device descriptor read/64, error -110 #define ETIMEDOUT 110 /* Connection timed out */ 1.2.2 设置地址1234567USB总线复位。USB端点0输出中断。读端点0缓冲区8字节。0x00 0x05 0x06 0x00 0x00 0x00 0x00 0x00 USB标准输出请求：设置地址。地址为：0x06 写端点0缓冲区0字节。USB端点0输入中断。 1.2.3 SETUP数据包，基于新地址，重新获取设备描述符123456789USB端点0输出中断。读端点0缓冲区8字节。0x80 0x06 0x00 0x01 0x00 0x00 0x12 0x00 USB标准输入请求：获取描述符——设备描述符。写端点0缓冲区16字节。0x12 0x01 0x10 0x01 0x00 0x00 0x00 0x10 0x88 0x88 0x01 0x00 0x00 0x01 0x01 0x02 USB端点0输入中断。写端点0缓冲区2字节。0x03 0x01 二、描述符2.1 各描述符之间的关系设备描述符（Device Descriptor） 配置描述符（Configuration Descriptor） 接口描述符（Interface Descriptor） HID描述符（HID Device Descriptor） 报告描述符（Report Descriptor） 一个设备描述符可以包含多个配置描述符，通常1个一个配置描述符可以包含多个接口描述符。一个接口描述符可以包含多个端点描述符。 接口描述符跟着配置描述符走的，无法单独存在。 2.2 各描述符简介：2.2.1 设备描述符（Device Descriptor）Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 1.10 bDeviceClass 0 (Defined at Interface level) bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 16 idVendor 0x8888 idProduct 0x0001 bcdDevice 1.00 iManufacturer 1 iProduct 2 iSerial 3 bNumConfigurations 1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//USB设备描述符的定义code uint8 DeviceDescriptor[]= //设备描述符为18字节&#123;//bLength字段。设备描述符的长度为18(0x12)字节 0x12,//bDescriptorType字段。设备描述符的编号为0x01 0x01,//bcdUSB字段。这里设置版本为USB1.1，即0x0110。//由于是小端结构，所以低字节在先，即0x10，0x01。 0x10, 0x01,//bDeviceClass字段。我们不在设备描述符中定义设备类，//而在接口描述符中定义设备类，所以该字段的值为0。 0x00,//bDeviceSubClass字段。bDeviceClass字段为0时，该字段也为0。 0x00,//bDeviceProtocol字段。bDeviceClass字段为0时，该字段也为0。 0x00,//bMaxPacketSize0字段。PDIUSBD12的端点0大小的16字节。 0x10,//idVender字段。厂商ID号，我们这里取0x8888，仅供实验用。//实际产品不能随便使用厂商ID号，必须跟USB协会申请厂商ID号。//注意小端模式，低字节在先。 0x88, 0x88,//idProduct字段。产品ID号，由于是第一个实验，我们这里取0x0001。//注意小端模式，低字节应该在前。 0x01, 0x00,//bcdDevice字段。我们这个USB鼠标刚开始做，就叫它1.0版吧，即0x0100。//小端模式，低字节在先。 0x00, 0x01,//iManufacturer字段。厂商字符串的索引值，为了方便记忆和管理，//字符串索引就从1开始吧。 0x01,//iProduct字段。产品字符串的索引值。刚刚用了1，这里就取2吧。//注意字符串索引值不要使用相同的值。 0x02,//iSerialNumber字段。设备的序列号字符串索引值。//这里取3就可以了。 0x03,//bNumConfigurations字段。该设备所具有的配置数。//我们只需要一种配置就行了，因此该值设置为1。 0x01&#125;; 2.2.2 配置描述符（Configuration Descriptor）- 接口描述符（Interface Descriptor） - HID描述符（HID Device Descriptor）Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 34 bNumInterfaces 1 bConfigurationValue 1 iConfiguration 0 bmAttributes 0x80 (Bus Powered) MaxPower 100mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 1 bInterfaceClass 3 Human Interface Device bInterfaceSubClass 1 Boot Interface Subclass bInterfaceProtocol 2 Mouse iInterface 0 HID Device Descriptor: bLength 9 bDescriptorType 33 bcdHID 1.10 bCountryCode 33 US bNumDescriptors 1 bDescriptorType 34 Report wDescriptorLength 52 Report Descriptor: (length is 52) 略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114//USB配置描述符集合的定义//配置描述符总长度为9+9+9+7字节code uint8 ConfigurationDescriptor[9+9+9+7]=&#123; /***************配置描述符***********************/ //bLength字段。配置描述符的长度为9字节。 0x09, //bDescriptorType字段。配置描述符编号为0x02。 0x02, //wTotalLength字段。配置描述符集合的总长度， //包括配置描述符本身、接口描述符、类描述符、端点描述符等。 sizeof(ConfigurationDescriptor)&amp;0xFF, //低字节 (sizeof(ConfigurationDescriptor)&gt;&gt;8)&amp;0xFF, //高字节 //bNumInterfaces字段。该配置包含的接口数，只有一个接口。 0x01, //bConfiguration字段。该配置的值为1。 0x01, //iConfigurationz字段，该配置的字符串索引。这里没有，为0。 0x00, //bmAttributes字段，该设备的属性。由于我们的板子是总线供电的， //并且我们不想实现远程唤醒的功能，所以该字段的值为0x80。 0x80, //bMaxPower字段，该设备需要的最大电流量。由于我们的板子 //需要的电流不到100mA，因此我们这里设置为100mA。由于每单位 //电流为2mA，所以这里设置为50(0x32)。 0x32, /*******************接口描述符*********************/ //bLength字段。接口描述符的长度为9字节。 0x09, //bDescriptorType字段。接口描述符的编号为0x04。 0x04, //bInterfaceNumber字段。该接口的编号，第一个接口，编号为0。 0x00, //bAlternateSetting字段。该接口的备用编号，为0。 0x00, //bNumEndpoints字段。非0端点的数目。由于USB鼠标只需要一个 //中断输入端点，因此该值为1。 0x01, //bInterfaceClass字段。该接口所使用的类。USB鼠标是HID类， //HID类的编码为0x03。 0x03, //bInterfaceSubClass字段。该接口所使用的子类。在HID1.1协议中， //只规定了一种子类：支持BIOS引导启动的子类。 //USB键盘、鼠标属于该子类，子类代码为0x01。 0x01, //bInterfaceProtocol字段。如果子类为支持引导启动的子类， //则协议可选择鼠标和键盘。键盘代码为0x01，鼠标代码为0x02。 0x02, //iConfiguration字段。该接口的字符串索引值。这里没有，为0。 0x00, /******************HID描述符************************/ //bLength字段。本HID描述符下只有一个下级描述符。所以长度为9字节。 0x09, //bDescriptorType字段。HID描述符的编号为0x21。 0x21, //bcdHID字段。本协议使用的HID1.1协议。注意低字节在先。 0x10, 0x01, //bCountyCode字段。设备适用的国家代码，这里选择为美国，代码0x21。 0x21, //bNumDescriptors字段。下级描述符的数目。我们只有一个报告描述符。 0x01, //bDescritporType字段。下级描述符的类型，为报告描述符，编号为0x22。 0x22, //bDescriptorLength字段。下级描述符的长度。下级描述符为报告描述符。 sizeof(ReportDescriptor)&amp;0xFF, (sizeof(ReportDescriptor)&gt;&gt;8)&amp;0xFF, /**********************端点描述符***********************/ //bLength字段。端点描述符长度为7字节。 0x07, //bDescriptorType字段。端点描述符编号为0x05。 0x05, //bEndpointAddress字段。端点的地址。我们使用D12的输入端点1。 //D7位表示数据方向，输入端点D7为1。所以输入端点1的地址为0x81。 0x81, //bmAttributes字段。D1~D0为端点传输类型选择。 //该端点为中断端点。中断端点的编号为3。其它位保留为0。 0x03, //wMaxPacketSize字段。该端点的最大包长。端点1的最大包长为16字节。 //注意低字节在先。 0x10, 0x00, //bInterval字段。端点查询的时间，我们设置为10个帧时间，即10ms。 0x0A&#125;; 2.2.3 报告描述符（Report Descriptor）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111//USB报告描述符的定义code uint8 ReportDescriptor[]=&#123; //每行开始的第一字节为该条目的前缀，前缀的格式为： //D7~D4：bTag。D3~D2：bType；D1~D0：bSize。以下分别对每个条目注释。 //这是一个全局（bType为1）条目，选择用途页为普通桌面Generic Desktop Page(0x01) //后面跟一字节数据（bSize为1），后面的字节数就不注释了， //自己根据bSize来判断。 0x05, 0x01, // USAGE_PAGE (Generic Desktop) //这是一个局部（bType为2）条目，说明接下来的应用集合用途用于鼠标 0x09, 0x02, // USAGE (Mouse) //这是一个主条目（bType为0）条目，开集合，后面跟的数据0x01表示 //该集合是一个应用集合。它的性质在前面由用途页和用途定义为 //普通桌面用的鼠标。 0xa1, 0x01, // COLLECTION (Application) //这是一个局部条目。说明用途为指针集合 0x09, 0x01, // USAGE (Pointer) //这是一个主条目，开集合，后面跟的数据0x00表示该集合是一个 //物理集合，用途由前面的局部条目定义为指针集合。 0xa1, 0x00, // COLLECTION (Physical) //这是一个全局条目，选择用途页为按键（Button Page(0x09)） 0x05, 0x09, // USAGE_PAGE (Button) //这是一个局部条目，说明用途的最小值为1。实际上是鼠标左键。 0x19, 0x01, // USAGE_MINIMUM (Button 1) //这是一个局部条目，说明用途的最大值为3。实际上是鼠标中键。 0x29, 0x03, // USAGE_MAXIMUM (Button 3) //这是一个全局条目，说明返回的数据的逻辑值（就是我们返回的数据域的值啦） //最小为0。因为我们这里用Bit来表示一个数据域，因此最小为0，最大为1。 0x15, 0x00, // LOGICAL_MINIMUM (0) //这是一个全局条目，说明逻辑值最大为1。 0x25, 0x01, // LOGICAL_MAXIMUM (1) //这是一个全局条目，说明数据域的数量为三个。 0x95, 0x03, // REPORT_COUNT (3) //这是一个全局条目，说明每个数据域的长度为1个bit。 0x75, 0x01, // REPORT_SIZE (1) //这是一个主条目，说明有3个长度为1bit的数据域（数量和长度 //由前面的两个全局条目所定义）用来做为输入， //属性为：Data,Var,Abs。Data表示这些数据可以变动，Var表示 //这些数据域是独立的，每个域表示一个意思。Abs表示绝对值。 //这样定义的结果就是，第一个数据域bit0表示按键1（左键）是否按下， //第二个数据域bit1表示按键2（右键）是否按下，第三个数据域bit2表示 //按键3（中键）是否按下。 0x81, 0x02, // INPUT (Data,Var,Abs) //这是一个全局条目，说明数据域数量为1个 0x95, 0x01, // REPORT_COUNT (1) //这是一个全局条目，说明每个数据域的长度为5bit。 0x75, 0x05, // REPORT_SIZE (5) //这是一个主条目，输入用，由前面两个全局条目可知，长度为5bit， //数量为1个。它的属性为常量（即返回的数据一直是0）。 //这个只是为了凑齐一个字节（前面用了3个bit）而填充的一些数据 //而已，所以它是没有实际用途的。 0x81, 0x03, // INPUT (Cnst,Var,Abs) //这是一个全局条目，选择用途页为普通桌面Generic Desktop Page(0x01) 0x05, 0x01, // USAGE_PAGE (Generic Desktop) //这是一个局部条目，说明用途为X轴 0x09, 0x30, // USAGE (X) //这是一个局部条目，说明用途为Y轴 0x09, 0x31, // USAGE (Y) //这是一个局部条目，说明用途为滚轮 0x09, 0x38, // USAGE (Wheel) //下面两个为全局条目，说明返回的逻辑最小和最大值。 //因为鼠标指针移动时，通常是用相对值来表示的， //相对值的意思就是，当指针移动时，只发送移动量。 //往右移动时，X值为正；往下移动时，Y值为正。 //对于滚轮，当滚轮往上滚时，值为正。 0x15, 0x81, // LOGICAL_MINIMUM (-127) 0x25, 0x7f, // LOGICAL_MAXIMUM (127) //这是一个全局条目，说明数据域的长度为8bit。 0x75, 0x08, // REPORT_SIZE (8) //这是一个全局条目，说明数据域的个数为3个。 0x95, 0x03, // REPORT_COUNT (3) //这是一个主条目。它说明这三个8bit的数据域是输入用的， //属性为：Data,Var,Rel。Data说明数据是可以变的，Var说明 //这些数据域是独立的，即第一个8bit表示X轴，第二个8bit表示 //Y轴，第三个8bit表示滚轮。Rel表示这些值是相对值。 0x81, 0x06, // INPUT (Data,Var,Rel) //下面这两个主条目用来关闭前面的集合用。 //我们开了两个集合，所以要关两次。bSize为0，所以后面没数据。 0xc0, // END_COLLECTION 0xc0 // END_COLLECTION&#125;;//通过上面的报告描述符的定义，我们知道返回的输入报告具有4字节。//第一字节的低3位用来表示按键是否按下的，高5位为常数0，无用。//第二字节表示X轴改的变量，第三字节表示Y轴的改变量，第四字节表示//滚轮的改变量。我们在中断端点1中应该要按照上面的格式返回实际的//鼠标数据。 三、QAQ:lsusb获取Report Descriptor异常A:Report Descriptors: UNAVAILABLE 获取Report Descriptorshttp://www.slashdev.ca/2010/05/08/get-usb-report-descriptor-with-linux/]]></content>
  </entry>
  <entry>
    <title><![CDATA[USB学习]]></title>
    <url>%2F2020%2F07%2F27%2FUSB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[USB学习1. 动机从事汽车电子行业以来，了解到了诸多之前没有接触过的领域，对于一个一直从事中间件开发的我来说，受益匪浅。工作的中心主要是手机互联相关业务，所涉及的领域比较多，多媒体，gstreamer，alsa，USB驱动，iap协议等。自觉除了USB驱动，其他内容都有略知一二，出问题都能从容应对，唯有USB驱动仍是软肋，一直在心中作梗。遂借工作之余，打算探个究竟。 2. 方法应用出生，当然是从应用入手，大概了解了内核驱动实现之后，觉得对USB的理解还是稍有欠缺。遂想到一句古话：纸上得来终觉浅，绝知此事要躬行。 USB in a NutShell https://www.beyondlogic.org/usbnutshell/usb1.shtml 大概了解USB的整体框架 《圈圈教你玩USB》 不可多得的好书，不过并不适合的零基础的人。要有51单片机的基础，USB相关知识。 参考该书实现一个USB外设，对USB有了更深的了解。做一个外设并不容易，还需要参考 《PDIUSBD12》和《AT89C52》手册，弄懂芯片的协议。 《Linux那些事儿之我是USB》 说实话，作者可能很懂USB，但是讲的内容并不像书的名字这么通俗易懂，可能驱动层就是这么难懂吧。可以协助阅读内核相关代码。 《usb_20》 当然，终极目标解释理解spec了，其他都是实现，这个还是内功。 3. 感悟说实话，互联网所谓的全栈工程师，跟嵌入式相比，感觉还是小巫见大巫了。单讲USB，从应用 - 驱动 - USB器件 - MCU(固件) - 设备(LED)。没有点基础还真吃不消。USB整体框图：]]></content>
      <tags>
        <tag>USB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[独立事件]]></title>
    <url>%2F2019%2F03%2F19%2F%E7%8B%AC%E7%AB%8B%E4%BA%8B%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Find the probability of getting exactly two “head” when flipping three coins. HHH THH HHT THT HTH TTH HTT TTT $$p(2’H’) = \frac{3}{8}$$ 计算骰子的概率两个骰子一次投掷得到的数值和：方法1：枚举| | 1 | 2 | 3 | 4 | 5 | 6 || —- | —- | —- | —- | —- | —- | —- || 1 | 2 | 3 | 4 | 5 | 6 | 7 || 2 | 3 | 4 | 5 | 6 | 7 | 8 || 3 | 4 | 5 | 6 | 7 | 8 | 9 || 4 | 5 | 6 | 7 | 8 | 9 | 10 || 5 | 6 | 7 | 8 | 9 | 10 | 11 || 6 | 7 | 8 | 9 | 10 | 11 | 12 |P(7) = 6/36方法2：P(5) = 4 / 36| D1 | D2 || —- | —- || 1 | 4 || 2 | 3 || 3 | 2 || 4 | 1 |]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[扑克中的概率和文氏图]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%89%91%E5%85%8B%E4%B8%AD%E7%9A%84%E6%A6%82%E7%8E%87%E5%92%8C%E6%96%87%E6%B0%8F%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[扑克中的概率P(Jack) = 4/52 = 1/13 P(♥) = 13/52 = 1/4 P(J and ♥) = 1/52 p(J or ♥) = (4+13-1)/52 = 16/52 = 4/13 文氏图1234567891011121314151617181920'''' # of Jacks / / +---------/--------------------------+ | / 52 | | /--/--\ | | | 4 | | | | /-----------\ | | \---|./ | | | ,'` 13 | | | _-` | | | | .' \--------\--/ | ,-` \ | # of J -` | \ | and ♥ | \ | +---------------------\--------------+ \ # of ♥ '''']]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[独立事件的组合概率]]></title>
    <url>%2F2019%2F03%2F19%2F%E7%8B%AC%E7%AB%8B%E4%BA%8B%E4%BB%B6%E7%9A%84%E7%BB%84%E5%90%88%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[独立事件的组合概率P(H)= 1/2 P(T) = 1/2 Two way to figure out the result1. 枚举出所有的情况P(H,H) = 1/4 (H,H ; H,T ; T,H ; T,T) 2. 每次事件都是独立的，计算独立事件都发生的概率P(H,H) = P(H) P(H) = (1/2) (1/2) = 1/4 P(T,H,T) = P(T) P(H) P(T) = (1/2) (1/2) (1/2) = 1/8]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本概率]]></title>
    <url>%2F2019%2F03%2F19%2F%E5%9F%BA%E6%9C%AC%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[ProbabilityP(H) = # of possibility that meet by condition / # of equally likely possibility]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯定理]]></title>
    <url>%2F2019%2F03%2F19%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86%2F</url>
    <content type="text"><![CDATA[贝叶斯定理:$$P(b|a) = \frac{P(a|b)P(b)}{P(a)}$$证明：$P(a \cap b) = P(a|b)P(b)$$P(a \cap b) = P(b \cap a) = P(b|a)P(a)$$P(a|b)P(b) = P(b|a)P(a)$$P(b|a) = \frac{P(a|b)P(b)}{P(a)}$ Ex1:一个袋子里有：5个均匀的硬币，10个不均匀的硬币(0.8 H, 0.2 T)求：取6枚4枚是Head，都是去的均匀硬币的概率。即：P(Fair|4/6 Heads)$P(Fair|4/6 Heads) = \frac{P(4/6 Heads|Fair)P(Fair)}{P(4/6 Heads)}$ Ex2:一个班上有30个学生，至少2个同学生日是同一天的概率。解：P(至少2个同学生日是同一天) = 1 - P(没有同学生日在同一天)P(没有同学生日在同一天) = $\frac{A^{30}_{365}}{365^{30}}$]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率相加]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%A6%82%E7%8E%87%E7%9B%B8%E5%8A%A0%2F</url>
    <content type="text"><![CDATA[概率相加P(A or B) = P(A) + P(B) - P(A and B) 文氏图12345678910111213141516171819''' +-----------------------------------------+ | | | /----------------\ | | | | | | | A | | | | /--------------------\ | | | | A | | | | | | and | B | | | | | B | | | | | | | | | | \----------|-----/ | | | | | | | | | | | \--------------------/ | | | +-----------------------------------------+ ''']]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排列组合]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[定义排列排列的定义：从n个不同元素中，任取m(m≤n,m与n均为自然数,下同）个元素按照一定的顺序排成一列，叫做从n个不同元素中取出m个元素的一个排列；从n个不同元素中取出m(m≤n）个元素的所有排列的个数，叫做从n个不同元素中取出m个元素的排列数，用符号 A(n,m）表示。$$计算公式：A^m_n = n(n-1)(n-2)…（n-m+1）=\frac{n!}{(n-m)!}$$ 组合组合的定义：从n个不同元素中，任取m(m≤n）个元素并成一组，叫做从n个不同元素中取出m个元素的一个组合；从n个不同元素中取出m(m≤n）个元素的所有组合的个数，叫做从n个不同元素中取出m个元素的组合数。用符号 C(n,m) 表示。$$计算公式：C^m_n =C^n_r=C^n_{n-r}=\frac{A^m_n}{m!}=\frac{n!}{m!(n-m)!};C(n,m)=C(n,n-m).n&gt;=m\\C^r_n =\binom{n}{r}=\dbinom{n}{n-r}$$]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相依事件的概率]]></title>
    <url>%2F2019%2F03%2F19%2F%E7%9B%B8%E4%BE%9D%E4%BA%8B%E4%BB%B6%E7%9A%84%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[剩下的35张取到1的概率36张卡片，分别1-9的四种花色(方块，红桃，梅花，黑桃)的卡片。随机取出9张，其中有四张是1的概率是多少？ 解法1P(all 4 cards is 1 in my hand of 9) = (# of ways in which event can happened)/(the total # of hands) total # of hands = $\frac{363534333231302928}{987654321}$ the # of hands with 4 1’s = $\frac{11113231302928}{54321}$ P(all 4 cards is 1 in my hand of 9) = $\frac{11113231302928}{54321} \frac{987654321}{3635343332313029*28}$= $\frac{2}{935}$ 解法2 取到一张1的概率 剩下的35张取到1的概率 剩下的34张取到1的概率 剩下的33张取到1的概率 $\frac{1}{36}*9$ $\frac{1}{35}*8$ $\frac{1}{34}*7$ $\frac{1}{33}*6$]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以二项式系数概括]]></title>
    <url>%2F2019%2F03%2F19%2F%E4%BB%A5%E4%BA%8C%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%E6%A6%82%E6%8B%AC%2F</url>
    <content type="text"><![CDATA[Fair Coin: Flip 4 times获得1个Head的概率P(exactly 1 “heads”) = P(HTTT) +P(THTT)+P(TTHT)+P(TTTH)= $\frac{1}{16}$+$\frac{1}{16}$+$\frac{1}{16}$+$\frac{1}{16}$=1/4 获得2个Head的概率4次里面选2次 $C_4^2=\frac{43}{21} = 6$P(exactly 2 “heads”) =$\frac{C_4^2}{16}$ = 3/8 Fair Coin: Flip 5 flips获得3个Head的概率5次里面选3次 $C_5^3=\frac{543}{321} = 10$P(exactly 3 “heads”) =$\frac{C_5^3}{32} $ = 5/16 Fair Coin: Flip n flips获得k个Head的概率n次里面选k次 $C_n^k= \frac{\frac{n!}{(n-k!)}}{k!}=\frac{n(n-1)…(n-k+1)}{k(k-1)…1}$all the probability = $2^{n}$P(exactly k “heads”) =$\frac{n(n-1)…(n-k+1)}{2^{n}k*(k-1)…1}$]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i.Max6移植Movidius]]></title>
    <url>%2F2019%2F01%2F14%2Fi.Max6%E7%A7%BB%E6%A4%8DMovidius%2F</url>
    <content type="text"><![CDATA[0、概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;时隔半年有余，由于工作的原因没能更新机器学习方面的内容。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之间林林总总，看完了《UNIX网络编程卷1：套接字联网API（第3版）》，《Linux多线程服务端编程：使用muduo C++网络库》和《Netty权威指南》的部分章节，完成了Tbox Telemetics模块的设计和编码，在这之前其实已对网络编程有所了解，也尝试用MFC编写过类QQ的sample，只是功能简单，单客户端/单服务端之间的通信，然真正用于网络通信的协议未曾染指。网络编程方面的知识，在这次的开发工作中得以锻炼，略有心得一二。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更换项目的间隙，中间有空闲二周，正直双十一，遂购架构书籍数本，看完《架构整洁之道》开始对于软件架构有了进一步的了解，科室读书会分享之，感觉功力倍增。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新年交替之际，开始AI实验室的预研，主要针对边缘计算，驾驶行为分析。嵌入式的硬件限制，只能外挂计算棒，开始研究Intel的Movidius，接触到了OpenVINO工具套件。本篇主要就是介绍一下NCS2在树莓派上的移植。 1、环境搭建 树莓派。系统Raspbian* 9 OS，官网下载最新版本即可。 The Intel™ Distribution of OpenVINO™ for Raspbian OS package to download is here and the Installation document here. 2、步骤IntroductionThis guide applies to 32-bit Raspbian 9 OS, which is an official OS for Raspberry Pi boards. IMPORTANT: All steps in this guide are required unless otherwise stated. The Intel® Distribution of OpenVINO™ toolkit for Raspbian* OS includes the MYRIAD plugin &gt;only. You can use it with the Intel® Movidius™ Neural Compute Stick (Intel® NCS) or the Intel® &gt;Neural Compute Stick 2 plugged in one of USB ports. ###About the Intel® Distribution of OpenVINO™ ToolkitThe Intel® Distribution of OpenVINO™ toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the toolkit extends computer vision (CV) workloads across Intel® hardware, maximizing performance. The Intel Distribution of OpenVINO toolkit includes the Intel® Deep Learning Deployment Toolkit (Intel® DLDT). ###Included in the Installation PackageThe Intel Distribution of OpenVINO toolkit for Raspbian OS is an archive with pre-installed header files and libraries. The following components are installed by default: Component Description Inference Engine This is the engine that runs the deep learning model. It includes a set of libraries for an easy inference integration into your applications. OpenCV* version 4.0 OpenCV* community version compiled for Intel® hardware. Sample Applications A set of simple console applications demonstrating how to use the Inference Engine in your applications. ###System Requirements ###Hardware: Raspberry Pi* board with ARMv7-A CPU architectureOne of Intel® Movidius™ Visual Processing Units (VPU):Intel® Movidius™ Neural Compute StickIntel® Neural Compute Stick 2 ###Operating Systems: Raspbian* Stretch, 32-bit Install the PackageOpen the Terminal* or your preferred console application.Go to the directory in which you downloaded the Intel Distribution of OpenVINO toolkit. This document assumes this is your ~/Downloads directory. If not, replace ~/Downloads with the directory where the file is located. cd ~/Downloads/ By default, the package file is saved as l_openvino_toolkit_ie_p_.tgz.Unpack the archive: tar -xf l_openvino_toolkit_ie_p_.tgz Modify the setupvars.sh script by replacing with the absolute path to the installation folder: sed -i “s||$(pwd)/inference_engine_vpu_arm|” inference_engine_vpu_arm/bin/setupvars.shNow the Intel Distribution of OpenVINO toolkit is ready to be used. Continue to the next sections to configure the environment and set up USB rules. Set the Environment VariablesYou must update several environment variables before you can compile and run Intel Distribution of OpenVINO toolkit applications. Run the following script to temporarily set the environment variables: source inference_engine_vpu_arm/bin/setupvars.sh (Optional) The Intel Distribution of OpenVINO environment variables are removed when you close the shell. As an option, you can permanently set the environment variables as follows: Open the .bashrc file in : vi /.bashrc Add this line to the end of the file: source ~/Downloads/inference_engine_vpu_arm/bin/setupvars.sh Save and close the file: press Esc and type :wq.To test your change, open a new terminal.You will see the following: [setupvars.sh] OpenVINO environment initialized Add USB RulesAdd the current Linux user to the users group: sudo usermod -a -G users “$(whoami)” Log out and log in for it to take effect. To perform inference on the Intel® Movidius™ Neural Compute Stick or Intel® Neural Compute Stick 2, install the USB rules as follows: sh inference_engine_vpu_arm/install_dependencies/install_NCS_udev_rules.sh Build and Run Object Detection SampleFollow the next steps to run pre-trained Face Detection network using samples from Intel Distribution of OpenVINO toolkit: Go to the folder with samples source code: cd inference_engine_vpu_arm/deployment_tools/inference_engine/samples Create build directory: mkdir build &amp;&amp; cd build Build the Object Detection Sample: cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=”-march=armv7-a”make -j2 object_detection_sample_ssd Download the pre-trained Face Detection model or copy it from a host machine:To download the .bin file with weights: wget –no-check-certificate https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-adas-0001/FP16/face-detection-adas-0001.bin To download the .xml file with the network topology: wget –no-check-certificate https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-adas-0001/FP16/face-detection-adas-0001.xml Run the sample with specified path to the model:Copy Code ./armv7l/Release/object_detection_sample_ssd -m face-detection-adas-0001.xml -d MYRIAD -i Run Face Detection Model Using OpenCV* APITo validate OpenCV installation, you may try to run OpenCV’s deep learning module with Inference Engine backend. Here is a Python sample, which works with Face Detection model: Download the pre-trained Face Detection model or copy it from a host machine:To download the .bin file with weights: wget –no-check-certificate https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-adas-0001/FP16/face-detection-adas-0001.bin To download the .xml file with the network topology: wget –no-check-certificate https://download.01.org/openvinotoolkit/2018_R4/open_model_zoo/face-detection-adas-0001/FP16/face-detection-adas-0001.xml Create a new Python file named as openvino_fd_myriad.py and copy the following script there:12345678910111213141516171819202122232425262728import cv2 as cv# Load the model net = cv.dnn.readNet('face-detection-adas-0001.xml', 'face-detection-adas-0001.bin') # Specify target device net.setPreferableTarget(cv.dnn.DNN_TARGET_MYRIAD) # Read an image frame = cv.imread('/path/to/image') # Prepare input blob and perform an inference blob = cv.dnn.blobFromImage(frame, size=(672, 384), ddepth=cv.CV_8U) net.setInput(blob) out = net.forward() # Draw detected faces on the frame for detection in out.reshape(-1, 7): confidence = float(detection[2]) xmin = int(detection[3] * frame.shape[1]) ymin = int(detection[4] * frame.shape[0]) xmax = int(detection[5] * frame.shape[1]) ymax = int(detection[6] * frame.shape[0]) if confidence &gt; 0.5: cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(0, 255, 0))# Save the frame to an image file cv.imwrite('out.png', frame) Run the script: python3 openvino_fd_myriad.py In this script, OpenCV* loads the Face Detection model in the Intermediate Representation (IR) format and an image. Then it runs the model and saves an image with detected faces. 3、i.Max6移植&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;理论上树莓派上编译的产物可以直接拷贝到相同处理器架构的i.Max6上(CPU都是ARMv7)，不过由于各个公司BSP对内核裁剪和集成编译的时候包含的软件包不同，需要修改的内容也不尽相同。这里写的是本司的开发板环境缺少的依赖： python3 lsb_release libstdc++ -&gt; 3.4.22 vi inference_engine_vpu_arm/install_dependencies/install_NCS_udev_rules.sh:%s/sudo//g #删除所有的sudo touch /etc/ld.so.conf 4、总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初步了解OpenVINO发现其是一大宝藏，里面包含了几十种流行模型的，并且有大量sample可以参考学习，可以通过组合各个模型实现自己想要的功能。增加对机器学习的实战经验，加深对机器学习的理解。遗憾的是Intel的NCS2目前貌似只支持深度学习的预测，之于一般的机器学习模型是否也支持，有待考证。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HelloWorld驱动模块]]></title>
    <url>%2F2018%2F09%2F17%2FHelloWorld%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[0、概述本章主要通过一个简单的实例，实现驱动模块的加载和卸载，并不实现具体的功能。 1、介绍1.1、编写hello.c1234567891011121314151617#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;MODULE_LICENSE("Dual BSD/GPL");static int hello_init(void)&#123; printk(KERN_ALERT "Hello, world\n");&#125;static void hello_exit(void)&#123; printk(KERN_ALERT "Goodbye, cruel world\n");&#125;module_init(hello_init);module_exit(hello_exit); 这个模块定义了两个函数，一个在模块加载到内核时被调用（hello_init），一个在模块去除时被调用（hello_exit）。module_init和module_exit这几行使用了特别的内核宏来指出这两个函数的角色。另一个特别的宏（MODULE_LICENSE）是用来告知内核，该模块带有一个自由的许可证，没有这样的说明，在模块加载时内核会报错。 printk函数在Linux内核中定义并且对模块可用，它与标准C库函数printf的行为相似。内核需要它自己的打印函数，因为没有C库的支持。字串KERN_ALERT是消息的优先级。在此模块中指定了一个高优先级，因为使用默认优先级的消息可能不会直接显示，这依赖于运行的内核版本、klogd守护进程的版本以及配置。 1.2、编写Makefile为了编译模块文件，有两种方法创建Makefile文件可以实现:1、只需一行即可，命令如下： obj-m := hello.o obj-m指出将要编译成的内核模块列表。.o 格式文件会自动地由相应的 .c 文件生成（不需要显式地罗列所有源代码文件）如果要把上述程序编译为一个运行时加载和删除的模块，则编译命令如下所示。 make -C /usr/src/kernels/2.6.25-14.fc9.i686 M=$PWD modules 这个命令首先是改变目录到用 -C 选项指定的位置（即内核源代码目录，这个参数要根据自己的情况而定）。这个 M= 选项使Makefile在构造modules目标前，返回到模块源码目录。然后，modules目标指向obj-m变量中设定的模块。这里的编译规则的意思是：在包含内核源代码位置的地方进行make，然后再编译 $PWD （当前）目录下的modules。这里允许我们使用所有定义在内核源代码树下的所有规则来编译我们的内核模块。 2、使用下面的Makefile来实现：12345678ifneq ($(KERNELRELEASE),) obj-m := hello.oelse KERNELDIR ?= /lib/modules/$(shell uname -r)/build PWD :=$(shell pwd)default: $(MAKE) -C $(KERNELDIR) M=$(PWD) modulesendif 然后保存后，使用make命令。编译完毕之后，就会在源代码目录下生成hello.ko文件，这就是内核驱动模块了。我们使用下面的命令来加载hello模块。 1.3、效果 dmesg | tail 这时，在终端里就会打印出内核信息了。同时，也可以使用lsmod命令来查看是否有加载了 xuleilx@xuleilx-MS-7817:/opt# lsmodModule Size Used byhelloworld 12448 0 至此，一个最简单的内核模块驱动程序就完成了。^_^ 2、总结至此，打开通往内核的大门。 下一步计划：编写字符设备驱动，可以存放一些数值。]]></content>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AndroidFramework学习之旅]]></title>
    <url>%2F2018%2F09%2F11%2FAndroidFrameWork%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85%2F</url>
    <content type="text"><![CDATA[作为一名开发者，Linux内核源码必定是其追求的终极目标。自觉愚钝，一直未敢染指Linux 内核源码。工作八年有余，解决Bug无数，设计模式，重构，STL库等也熟知一二，然对于一 些涉及底层的问题未能深入剖析，自觉功力不足。 近几年机器学习嫣然成为风口浪尖的弄潮儿，于是自学半年有余，略有心得，自觉入门。然在 实际工作和运用中缺乏实战机会，且与自己的职业生涯规划略有出入，遂放缓脚步，寻找更好 的学习点。 Android系统一直是Linux开发者鄙夷的对象，愚随大流，以勉Android代码为耻，然 Android数年以来表现依然如此强势，尤近年来车载行业对于Android系统的接纳，传统车载 行业必然会受到一定的冲击，Android系统的架构也是经受过数年的考验，其架构的成熟性以 及通用性耳目共睹，遂决定开始学习Android系统架构。 于是开始我的Android系统的学习之旅，对于学习这件事，随着年龄的增长，也有了自己独特 的一套方法。一般会去知乎上搜索一些对于一门技术的学习方法或是推荐书籍，于是检索到： https://www.zhihu.com/question/19759722 看完这些文章所说的对于C/C++和Linux系统知识 的要求后信心倍增，自觉对于这些方面的知识很有自信。 第一阶段打算跟着&quot;老罗的Android之旅&quot;，学完之后也应该有自己的一套知识体系了。先从内 核入手，不求甚解，能够摸索出内核代码的阅读方法，就已经足够，也不指望看一遍书就能摸 透。根据 老罗的Android之旅 提到的《Linux Kernel Development》、 《Understanding the Linux Kernel》、《Linux Device Drivers》和 《Linux内核源代码情景分析》开始入手内核。 分成两个阶段：内核学习，安卓FrameWork 第一阶段，开始我的内核学习之旅： 1.编译Android系统代码 2018/08/27 ~ 2.《Linux Kernel Development》 2018/08/27 ~ 2018/09/07 3.字符设备驱动程序 2018/09/11 ~ 4.《LDD3》 2018/09/11 ~]]></content>
      <tags>
        <tag>AndroidFramework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataVisualisation]]></title>
    <url>%2F2018%2F06%2F28%2FDataVisualisation%2F</url>
    <content type="text"><![CDATA[0、概述本阶段完成了关于数据可视化的学习，这部分的学习为我打开了一扇通往新世界的大门。 一个人如果能再某个领域成为专家就已经是一件很了不起的事情了，对于MachineLearner来说，将会面对不同领域的问题，需要具备不同的DomainKnowledge是一件几乎不可能的事情，可能我们通过几周或者甚至几天的学习，对问题领域有个大概的了解，但是对不同的Feature之间的关系，影响可能就无法知晓了，当然我们可以通过咨询相应领域的专家，不过这实际上也是一件相当不易的事情。 数据可视化就想武器中的瑞士军刀，它将数据以图表的形式展现在MachineLearner的面前，通过观察图表，我们可以知道数据与数据之间的联系，可以连接到数据的走上，范围，概率分布等。通过这些，我们可以方便的挑选适合我们模型的Features。 1、思维导图 2、使用方法2.1、Univariate plotting with pandas1234567891011121314151617181920import pandas as pdreviews = pd.read_csv("../input/wine-reviews/winemag-data_first150k.csv", index_col=0)reviews.head(3)# Bar Chat# Good for nominal and small ordinal categorical data.reviews['province'].value_counts().head(10).plot.bar()(reviews['province'].value_counts().head(10) / len(reviews)).plot.bar()reviews['points'].value_counts().sort_index().plot.bar()# Line charts# Good for ordinal categorical and interval data.reviews['points'].value_counts().sort_index().plot.line()# Area charts# Good for ordinal categorical and interval data.reviews['points'].value_counts().sort_index().plot.area()# Histogram# Good for interval data.reviews[reviews['price'] &lt; 200]['price'].plot.hist() 2.2、Bivariate plotting with pandas1234567891011121314151617181920212223import pandas as pdreviews = pd.read_csv("../input/wine-reviews/winemag-data_first150k.csv", index_col=0)reviews.head()# Scatter Plot# Good for interval and some nominal categorical data.reviews[reviews['price'] &lt; 100].sample(100).plot.scatter(x='price', y='points')# Hex Plot# Good for interval and some nominal categorical data.reviews[reviews['price'] &lt; 100].plot.hexbin(x='price', y='points', gridsize=15)# Stacked Bar Chart# Good for nominal and ordinal categorical data.wine_counts = pd.read_csv("../input/most-common-wine-scores/top-five-wine-score-counts.csv",index_col=0)wine_counts.head()wine_counts.plot.bar(stacked=True)wine_counts.plot.area()# Bivariate Line Chart# Good for ordinal categorical and interval data.wine_counts.plot.line() 2.3、Multivariate plotting with pandas123456789101112131415161718192021222324252627282930import pandas as pdpd.set_option('max_columns', None)df = pd.read_csv("../input/fifa-18-demo-player-dataset/CompleteDataset.csv", index_col=0)import reimport numpy as npfootballers = df.copy()footballers['Unit'] = df['Value'].str[-1]footballers['Value (M)'] = np.where(footballers['Unit'] == '0', 0, footballers['Value'].str[1:-1].replace(r'[a-zA-Z]',''))footballers['Value (M)'] = footballers['Value (M)'].astype(float)footballers['Value (M)'] = np.where(footballers['Unit'] == 'M', footballers['Value (M)'], footballers['Value (M)']/1000)footballers = footballers.assign(Value=footballers['Value (M)'], Position=footballers['Preferred Positions'].str.split().str[0])# Parallel Coordinatesfrom pandas.plotting import parallel_coordinatesf = ( footballers.iloc[:, 12:17] .loc[footballers['Position'].isin(['ST', 'GK'])] .applymap(lambda v: int(v) if str.isdecimal(v) else np.nan) .dropna())f['Position'] = footballers['Position']f = f.sample(200)parallel_coordinates(f, 'Position') 2.4、Plotting with seaborn123456789101112131415161718192021222324252627282930313233343536import pandas as pdreviews = pd.read_csv("../input/wine-reviews/winemag-data_first150k.csv", index_col=0)import seaborn as sns# Count (Bar) Plot# Good for nominal and small ordinal categorical data.sns.countplot(reviews['points'])# KDE Plot# Good for interval data.sns.kdeplot(reviews.query('price &lt; 200').price)sns.kdeplot(reviews[reviews['price'] &lt; 200].loc[:, ['price', 'points']].dropna().sample(5000))# Distplotsns.distplot(reviews['points'], bins=10, kde=False)# Joint (Hex) Plot# Good for interval and some nominal categorical data.sns.jointplot(x='price', y='points', data=reviews[reviews['price'] &lt; 100])sns.jointplot(x='price', y='points', data=reviews[reviews['price'] &lt; 100], kind='hex', gridsize=20)# Boxplot and violin plot# Good for interval data and some nominal categorical data.df = reviews[reviews.variety.isin(reviews.variety.value_counts().head(5).index)]sns.boxplot( x='variety', y='points', data=df)sns.violinplot( x='variety', y='points', data=reviews[reviews.variety.isin(reviews.variety.value_counts()[:5].index)]) 2.5、Faceting with seaborn1234567891011121314151617# Facet Grid# Good for data with at least two categorical variables.import seaborn as snsdf = footballers[footballers['Position'].isin(['ST', 'GK'])]g = sns.FacetGrid(df, col="Position")g.map(sns.kdeplot, "Overall")g = sns.FacetGrid(df, col="Position", col_wrap=6)g.map(sns.kdeplot, "Overall")df = df[df['Club'].isin(['Real Madrid CF', 'FC Barcelona', 'Atlético Madrid'])]g = sns.FacetGrid(df, row="Position", col="Club")g.map(sns.violinplot, "Overall")# Pair Plot# Good for exploring most kinds of data.sns.pairplot(footballers[['Overall', 'Potential', 'Value']]) 2.6、Multivariate plotting with seaborn1234567891011121314151617181920212223# Multivariate Scatter Plotsns.lmplot(x='Value', y='Overall', markers=['o', 'x', '*'], hue='Position', data=footballers.loc[footballers['Position'].isin(['ST', 'RW', 'LW'])], fit_reg=False )# Grouped Box Plotf = (footballers .loc[footballers['Position'].isin(['ST', 'GK'])] .loc[:, ['Value', 'Overall', 'Aggression', 'Position']] )f = f[f["Overall"] &gt;= 80]f = f[f["Overall"] &lt; 85]f['Aggression'] = f['Aggression'].astype(float)sns.boxplot(x="Overall", y="Aggression", hue='Position', data=f)# Heatmapf = ( footballers.loc[:, ['Acceleration', 'Aggression', 'Agility', 'Balance', 'Ball control']] .applymap(lambda v: int(v) if str.isdecimal(v) else np.nan) .dropna()).corr()sns.heatmap(f, annot=True) 3、总结通过这一阶段的学习，掌握了主流的python库pandas，seaborn的绘图方法，之前学习的matplotlib的知识也在这段时间稍微复习了一下，希望在今后的实践中好好运用数据可视化这部分的知识。并且帮助评价训练出来的模型。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas总结]]></title>
    <url>%2F2018%2F06%2F25%2Fpandas%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[0、概述 这段时间主要学习了kaggle网站上的“pandas”的部分。俗话说的好：学过的东西通过 自己的理解转换成自己的东西才算是学会。所以说每学完一个知识点都要做一个总结，依赖理 清自己的只是结构，而来温故而知新。网址：https://www.kaggle.com/learn/pandas 最近刚从知乎上看到一个帖子，主要是讲如何学习，成为别人眼中的学霸。总结了一下主 要有以下四点（排名不分先后）：1、自律 2、目标 3、计划 4、理解。所以逼着自己学 完一个知识点写总结，旅游要写游记。与自己死磕到底。 1、思维导图 2、使用方法 本阶段刚刚接触Pandas不久，给我总体的印象是不需要专门的学习，pandas只是工 具，需要的时候可以网上查询，一般的需求这样就可以满足了。如果想对数据进行分析就需要 系统的学习一下，很多功能网上说的不清楚，一些学的半斤八两的人喜欢发这些玩意儿，学一 下官网的Tutorials和Cookbook。网址：http://pandas.pydata.org/pandas-docs/stable/ 3、总结 回想4,5年前在FNST使用Ruby制作语音播报工具，学到的东西还是挺多的，接触到了 CSV/EXCEL文件操作，面向对象语言的进一步理解，lambda，无数据类型语言，TTS等。现 在感叹为啥不适用Python啊，现在就不要学了，不过Python和Ruby很像，学起来很快就上 手了。CSV/EXCEL文件操作不就是现在学的pandas嘛。哈哈~ 学无止境，学海无涯。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习总结]]></title>
    <url>%2F2018%2F06%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[0、概述 这段时间主要学习了kaggle网站上的“机器学习”的部分。之前很长一段时间都是在学习 理论，没有机会实践，Kaggle是个不错的平台，有很多使用的难题等待着全世界聪敏的头脑 去解决。通过本篇的学习，很好的将理论知识用到了实践当中，比如之前学习到的绘制模型在 样本集和测试集上的准确率，防止模型过拟合和欠拟合，选择最优的点。还有之前不明白交叉 验证的作用，以及实际效果。网址：https://www.kaggle.com/learn/machine-learning 1、思维导图 2、步骤2.1、分析数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354############## 观察数据 ##############import pandas as pd# save filepath to variable for easier accessmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'# read the data and store data in DataFrame titled melbourne_datamelbourne_data = pd.read_csv(melbourne_file_path) # print a summary of the data in Melbourne dataprint(melbourne_data.describe())print(melbourne_data.columns)############## 选择过滤数据 ############### store the series of prices separately as melbourne_price_data.melbourne_price_data = melbourne_data.Price# the head command returns the top few lines of data.print(melbourne_price_data.head())columns_of_interest = ['Landsize', 'BuildingArea']two_columns_of_data = melbourne_data[columns_of_interest]two_columns_of_data.describe()y = melbourne_data.Pricemelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']X = melbourne_data[melbourne_predictors]############## Partial Dependence Plots ##############import pandas as pdfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifierfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependencefrom sklearn.preprocessing import Imputercols_to_use = ['Distance', 'Landsize', 'BuildingArea']def get_some_data(): data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') y = data.Price X = data[cols_to_use] my_imputer = Imputer() imputed_X = my_imputer.fit_transform(X) return imputed_X, y X, y = get_some_data()my_model = GradientBoostingRegressor()my_model.fit(X, y)my_plots = plot_partial_dependence(my_model, features=[0,2], X=X, feature_names=cols_to_use, grid_resolution=10)############## Data Leakage ############### Leaky Predictors# 数据集中包含预测时不可用的数据。比如，患有癌症的病人，使用了抗生素。# Leaky Validation Strategy# 检验策略有问题。或者测试集的数据影响了验证集数据。比如，调用train_test_split之前运行预处理（如为缺失值拟合Imputer）。 2.2、数据处理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697############## 检查数据是否有空项 ##############print(data.isnull().sum())############## 丢掉含有空的数据列 ##############cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]reduced_X_train = X_train.drop(cols_with_missing, axis=1)reduced_X_test = X_test.drop(cols_with_missing, axis=1)print("Mean Absolute Error from dropping columns with Missing Values:")print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))############## 填充空的数据列 ##############from sklearn.preprocessing import Imputermy_imputer = Imputer()imputed_X_train = my_imputer.fit_transform(X_train)imputed_X_test = my_imputer.transform(X_test)print("Mean Absolute Error from Imputation:")print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))############## 扩展填充空的数据列 ##############imputed_X_train_plus = X_train.copy()imputed_X_test_plus = X_test.copy()cols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())for col in cols_with_missing: imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull() imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()# Imputationmy_imputer = Imputer()imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)print("Mean Absolute Error from Imputation while Track What Was Imputed:")print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))############## 将非数值列One-Hot Encoding ############### Read the dataimport pandas as pdtrain_data = pd.read_csv('../input/train.csv')test_data = pd.read_csv('../input/test.csv')# Drop houses where the target is missingtrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)target = train_data.SalePrice# Since missing values isn't the focus of this tutorial, we use the simplest# possible approach, which drops these columns. # For more detail (and a better approach) to missing values, see# https://www.kaggle.com/dansbecker/handling-missing-valuescols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()] candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)# "cardinality" means the number of unique values in a column.# We use it as our only way to select categorical columns here. This is convenient, though# a little arbitrary.low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if candidate_train_predictors[cname].nunique() &lt; 10 and candidate_train_predictors[cname].dtype == "object"]numeric_cols = [cname for cname in candidate_train_predictors.columns if candidate_train_predictors[cname].dtype in ['int64', 'float64']]my_cols = low_cardinality_cols + numeric_colstrain_predictors = candidate_train_predictors[my_cols]test_predictors = candidate_test_predictors[my_cols]print(train_predictors.dtypes.sample(10))one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)from sklearn.model_selection import cross_val_scorefrom sklearn.ensemble import RandomForestRegressordef get_mae(X, y): # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention return -1 * cross_val_score(RandomForestRegressor(50), X, y, scoring = 'neg_mean_absolute_error').mean()predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])mae_without_categoricals = get_mae(predictors_without_categoricals, target)mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))############## 多个文件join ##############one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, join='left', axis=1) 2.3、选择模型1234567891011121314151617181920212223242526272829303132333435363738############## 决策树回归 ##############from sklearn.tree import DecisionTreeRegressor# Define modelmelbourne_model = DecisionTreeRegressor()# Fit modelmelbourne_model.fit(X, y)print("Making predictions for the following 5 houses:")print(X.head())print("The predictions are")print(melbourne_model.predict(X.head()))############## 随机森林 ##############from sklearn.ensemble import RandomForestRegressorfrom sklearn.metrics import mean_absolute_errorforest_model = RandomForestRegressor()forest_model.fit(train_X, train_y)melb_preds = forest_model.predict(val_X)print(mean_absolute_error(val_y, melb_preds))############## XGBoost ##############from xgboost import XGBRegressormy_model = XGBRegressor()# Add silent=True to avoid printing out updates with each cyclemy_model.fit(train_X, train_y, verbose=False)############## PipeLines ##############from sklearn.ensemble import RandomForestRegressorfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import Imputermy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())my_pipeline.fit(train_X, train_y)predictions = my_pipeline.predict(test_X) 2.4、验证模型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768############## 模型在测试集上的误差 ##############from sklearn.model_selection import train_test_split# split data into training and validation data, for both predictors and target# The split is based on a random number generator. Supplying a numeric value to# the random_state argument guarantees we get the same split every time we# run this script.train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)# Define modelmelbourne_model = DecisionTreeRegressor()# Fit modelmelbourne_model.fit(train_X, train_y)# get predicted prices on validation dataval_predictions = melbourne_model.predict(val_X)print(mean_absolute_error(val_y, val_predictions))############## 绘制不同个数叶子节点的误差 ##############from sklearn.metrics import mean_absolute_errorfrom sklearn.tree import DecisionTreeRegressordef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val): model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0) model.fit(predictors_train, targ_train) preds_val = model.predict(predictors_val) mae = mean_absolute_error(targ_val, preds_val) return(mae) # Data Loading Code Runs At This Pointimport pandas as pd # Load datamelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'melbourne_data = pd.read_csv(melbourne_file_path) # Filter rows with missing valuesfiltered_melbourne_data = melbourne_data.dropna(axis=0)# Choose target and predictorsy = filtered_melbourne_data.Pricemelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']X = filtered_melbourne_data[melbourne_predictors]from sklearn.model_selection import train_test_split# split data into training and validation data, for both predictors and targettrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)# compare MAE with differing values of max_leaf_nodesplot_x = []plot_y = []for max_leaf_nodes in np.arange(10,1000,5): my_mae = get_mae(max_leaf_nodes, train_X, test_X, train_y, test_y) print("Max leaf nodes: %d \t\t Mean Absolute Error: %d" %(max_leaf_nodes, my_mae)) plot_x.append(max_leaf_nodes) plot_y.append(my_mae)plt.plot(plot_x, plot_y)print("index:%f,nodes:%f"%(plot_y.index(min(plot_y)),plot_x[plot_y.index(min(plot_y))]))plt.show()############## Cross Validation ##############from sklearn.ensemble import RandomForestRegressorfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import Imputermy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())from sklearn.model_selection import cross_val_scorescores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')print('Mean Absolute Error %2f' %(-1 * scores.mean())) 3、总结机器学习是一种数据处理的科学，采用科学的分析方式调整模型的参数，试验的数据，达到提高识别率的目的。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DigitRecognizer]]></title>
    <url>%2F2018%2F06%2F12%2FDigitRecognizer%2F</url>
    <content type="text"><![CDATA[0、概述本篇主要记录本人在Kaggle的Digit Recognizer比赛中学习和用到的知识。 1、介绍 首先介绍一下数字识别(Digit Recognizer)，数字识别堪称机器学习领域的&quot;Hello World&quot;，几乎可以说是每个学习机器学习的入门指南。不过，这个入门还是有门槛的，不像 学习编程语言的&quot;Hello World&quot;，如果一开始就一头扎进来，可能会摸不着头脑。 一开始接触机器学习的是看到有朋友圈里有人发TensorFlow相关的东西，于是在0基础 的情况下，配置TensorFlow环境，模仿TensorFlow的例子开始了Digit Recognizer的 实践，发现做下来不知道自己在做什么，例子的每一步不知道是在干什么，一头雾水。 后来看了一位朋友的博客，分享的是他本人学习机器学习的心路历程，以及一些推荐的公 开课和书籍，撸完Coursera上吴恩达的视频后对机器学习有了初步认识，接着撸了李宏毅的 台大课程，感觉思路豁然开朗，顿时理解了吴恩达讲解的很多内容，毕竟是国语嘛，学起来就 是快。最近开始撸线代和概率论，但是总觉得缺点什么，想想大概是缺少实践吧。 寻找实践的突破口，发现项目中出现的Bug分配到各个担当是个费时费力的工作，如果能 够通过Bug描述自动分类那不就可以节省大量人力成本了嘛。于是说干就干，撸起膀子加油 干。文章分类很快想到了吴恩达老师讲的垃圾邮件分类问题，果断朴素贝叶斯算法。中文与英 文不一样，英文通过空格可以分词，中文就没有那么简单了，于是调研了一下，发现github 上有个很好的中文分分词库jieba。试了用一个项目的数据训练后去测试另一个项目的数据， 准确率也就在50%左右，这样的准确率是无法忍受的，至少在90%以上的准确率才能当做产品 使用吧。这个时候体会到了吴恩达老师的那句话，其实到最后机器学习工作者不是去实现什么 牛逼的算法，因为已经有一大批专门研究算法的人每天从事着这样的工作，用实现好的算法库 比自己实现的效率高，性能好。机器学习工作者主要的工作是：选择数据，选择模型，优化数 据，优化配置，提高准确率。 于是参加了Kaggle比赛，学习各路大神是如何玩转机器学习的。说了这么多，回到我们 的主题Digit Recognizer，分类的方法很多，我们将一一道来。 2、实现2.1、SVM 实现数字识别(scikit-learn)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn import svm,metrics# 将图片数据可视化def showImgByRow(row,num=1): ''' show pixel :param row:pixel row np.array :return: null ''' picture = row.reshape((num*28,28)) plt.imshow(picture,cmap='gray') plt.axis('off') plt.show()# 读取训练数据trainFile = r'digit-recognizer/train.csv'trainDF = pd.read_csv(trainFile)# 为了save your life，我们只取前500组数据。其实这样做是可能有问题的，# 如果数据不是随机分布的，前5000个数据都是某个数字，那就game over了# 所以无论做何种预测，最好打印出所有的数据分布。images = trainDF.iloc[0:5000,1:]labels = trainDF.iloc[0:5000,:1]train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.8, random_state=0)# 处理输入数据X = train_imagesy = train_labels.values.ravel()# regularization 正规化，归一化# 对于数字识别来说，像素点的为0与非0是完全不同的意义，# 如果取0~255，可能会让算法过渡关注数字的大小，导致识别率的下降X[X&gt;0] = 1 #X/=255# 创建分类器clf = svm.SVC(decision_function_shape='ovr',C = 7,gamma = 0.009)# 喂数据clf.fit(X, y)# 处理测试数据test_images[test_images&gt;0] = 1 #test_images/=255# 预测数据predicted = clf.predict(test_images)expected = test_labels#准确率print(clf.score(test_images,test_labels))print("Classification report for classifier %s:\n%s\n" % (clf, metrics.classification_report(expected, predicted)))print("Confusion matrix:\n%s" % metrics.confusion_matrix(y, y)) 2.2、MLP（DNN）实现数字识别(scikit-learn)1234from sklearn.neural_network import MLPClassifierclf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4, solver='sgd', verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1) 2.3、DNN实现数字识别(keras)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom keras.models import Sequentialfrom keras.layers import Dense,Activationfrom keras.utils import to_categorical# 读取训练数据trainFile = r'digit-recognizer/train.csv'trainDF = pd.read_csv(trainFile)# 为了save your life，我们只取前500组数据。其实这样做是可能有问题的，# 如果数据不是随机分布的，前5000个数据都是某个数字，那就game over了# 所以无论做何种预测，最好打印出所有的数据分布。images = trainDF.iloc[0:,1:]labels = trainDF.iloc[0:,:1]train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.9, random_state=0)# 处理输入数据X = imagesy = labels.values.ravel()# regularization 正规化，归一化# 对于数字识别来说，像素点的为0与非0是完全不同的意义，# 如果取0~255，可能会让算法过渡关注数字的大小，导致识别率的下降X/=255 #X[X&gt;0] = 1# 创建分类器clf = Sequential()clf.add(Dense(output_dim=64,input_dim=784))clf.add(Activation("relu"))clf.add(Dense(output_dim=10))clf.add(Activation("softmax"))clf.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# Convert labels to categorical one-hot encodingone_hot_labels = to_categorical(y, num_classes=10)# Train the model, iterating on the data in batches of 32 samples# 喂数据clf.fit(X, one_hot_labels, epochs=10, batch_size=32)# 处理测试数据test_images/=255 #test_images[test_images&gt;0] = 1# evaluate数据expected = test_labelsone_hot_test_labels = to_categorical(expected, num_classes=10)score = clf.evaluate(test_images, one_hot_test_labels)print(score) 2.4、CNN实现数字识别(keras)参考李宏毅老师的教学视频，自己实现的卷积神经网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom keras.models import Sequentialfrom keras.layers.core import Dense,Activationfrom keras.layers import MaxPooling2D,Convolution2D,Flattenfrom keras.utils.np_utils import to_categorical# 读取训练数据trainFile = r'digit-recognizer/train.csv'trainDF = pd.read_csv(trainFile)# 为了save your life，我们只取前500组数据。其实这样做是可能有问题的，# 如果数据不是随机分布的，前5000个数据都是某个数字，那就game over了# 所以无论做何种预测，最好打印出所有的数据分布。images = trainDF.iloc[0:5000,1:]labels = trainDF.iloc[0:5000,:1]train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.8, random_state=5)# 处理输入数据X = train_images.valuesy = train_labels.values.ravel()# regularization 正规化，归一化# 对于数字识别来说，像素点的为0与非0是完全不同的意义，# 如果取0~255，可能X会让算法过渡关注数字的大小，导致识别率的下降# 此处采用正规化处理，减去平局值，除以标准差mean_px = X.mean().astype(np.float32)std_px = X.std().astype(np.float32)def standardize(x): return (x-mean_px)/std_pxX = standardize(X)# 创建模型model = Sequential()# 25个filter，每个大小是3*3.这样会得到25张图片，去掉边角图片变成26*26model.add(Convolution2D(25,3,3,input_shape=(28,28,1)))# 用2*2的框去取最大值。13*13*25model.add(MaxPooling2D((2,2)))# 越接近Output Filter越多，图包含的信息越多。11*11*50model.add(Convolution2D(50,3,3))# 用2*2的框去取最大值。5*5*50. 比如：去掉一半的像素点，不会改变原图像。model.add(MaxPooling2D((2,2)))# 将图像拉直成1Dmodel.add(Flatten())# 以上是卷积过程，下面是DNN。100个neuron全连接model.add(Dense(output_dim=100))model.add(Activation('relu'))model.add(Dense(output_dim=10))model.add(Activation('softmax'))model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# Convert labels to categorical one-hot encodingone_hot_labels = to_categorical(y, num_classes=10)# Train the model, iterating on the data in batches of 32 samples# 喂数据model.fit(X.reshape(X.shape[0],28,28,1), one_hot_labels, epochs=30, batch_size=64)# 处理测试数据test_images = test_images.valuestest_images = standardize(test_images)# valuationexpected = test_labelsone_hot_test_labels = to_categorical(expected, num_classes=10)score = model.evaluate(test_images.reshape(test_images.shape[0],28,28,1), one_hot_test_labels)print(score) Kaggle上Digit_Recognizer 的Kernel中评分最高的实现，主要的改善有： Dropout 随机丢掉若干神经元 ReduceLROnPlateau当算法在最低点徘徊时，降低LR ImageDataGenerator手动创建更多的学习样本。★关键 分别在训练和测试集上绘制loss和accuracy，观察算法的学习情况。★调整学习方法的依据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimport seaborn as snsnp.random.seed(2)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import confusion_matriximport itertoolsfrom keras.utils.np_utils import to_categorical # convert to one-hot-encodingfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2Dfrom keras.optimizers import RMSpropfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.callbacks import ReduceLROnPlateausns.set(style='white', context='notebook', palette='deep')# Load the datatrain = pd.read_csv(r"digit-recognizer/train.csv")test = pd.read_csv(r"digit-recognizer/test.csv")Y_train = train["label"]# Drop 'label' columnX_train = train.drop(labels = ["label"],axis = 1)# free some spacedel train# g = sns.countplot(Y_train)Y_train.value_counts()# plt.show()# Check the dataprint(X_train.isnull().any().describe())print(test.isnull().any().describe())# Normalize the dataX_train = X_train / 255.0test = test / 255.0# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)X_train = X_train.values.reshape(-1,28,28,1)test = test.values.reshape(-1,28,28,1)# Encode labels to one hot vectors (ex : 2 -&gt; [0,0,1,0,0,0,0,0,0,0])Y_train = to_categorical(Y_train, num_classes = 10)# Set the random seedrandom_seed = 2# Split the train and the validation set for the fittingX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)# Some examples# g = plt.imshow(X_train[0][:,:,0])## plt.show()# Set the CNN model# my CNN architechture is In -&gt; [[Conv2D-&gt;relu]*2 -&gt; MaxPool2D -&gt; Dropout]*2 -&gt; Flatten -&gt; Dense -&gt; Dropout -&gt; Outmodel = Sequential()model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))model.add(MaxPool2D(pool_size=(2,2)))model.add(Dropout(0.25))model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(256, activation = "relu"))model.add(Dropout(0.5))model.add(Dense(10, activation = "softmax"))# Define the optimizeroptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)# Compile the modelmodel.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])# Set a learning rate annealerlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)epochs = 1 # Turn epochs to 30 to get 0.9967 accuracybatch_size = 64# With data augmentation to prevent overfitting (accuracy 0.99286)datagen = ImageDataGenerator( featurewise_center=False, # set input mean to 0 over the dataset samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=False, # divide inputs by std of the dataset samplewise_std_normalization=False, # divide each input by its std zca_whitening=False, # apply ZCA whitening rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180) zoom_range = 0.1, # Randomly zoom image width_shift_range=0.1, # randomly shift images horizontally (fraction of total width) height_shift_range=0.1, # randomly shift images vertically (fraction of total height) horizontal_flip=False, # randomly flip images vertical_flip=False) # randomly flip imagesdatagen.fit(X_train)# Fit the modelhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size , callbacks=[learning_rate_reduction])# Plot the loss and accuracy curves for training and validationfig, ax = plt.subplots(2,1)ax[0].plot(history.history['loss'], color='b', label="Training loss")ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])legend = ax[0].legend(loc='best', shadow=True)ax[1].plot(history.history['acc'], color='b', label="Training accuracy")ax[1].plot(history.history['val_acc'], color='r',label="Validation accuracy")legend = ax[1].legend(loc='best', shadow=True)# Plot the confusion_matrix# confusion_mtx = confusion_matrix(expected, predicted)# df_cm = pd.DataFrame(confusion_mtx, index = [i for i in range(0,10)], columns = [i for i in range(0,10)])# plt.figure(figsize = (6,5))# conf_mat = sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g', cbar = False)# conf_mat.set(xlabel='Predicts', ylabel='True')# plt.show() 2.5、kNN实现数字识别此处不再赘述，请参考前文：https://xuleilx.github.io/2018/04/12/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/ 3、总结机器学习把复杂的数字识别问题的变成了简单的分类问题。通过本文可以发现同一个问题可以 由不同的算法解决。本文主要用的是SVM、DNN、CNN、kNN四种算法，分别采用刚刚接触的 Scikit-learn和Keras实现。这里尤其要说明一点的是，神经网络将传统机器学习的函数模 型选择问题变成了搭建神经网络结构的问题。通过对loss和accuracy在样本集和测试集上的 表现权衡bias和variance，成功图像识别问题变成了数据分析问题。这大概就是机器学习的 魅力所在，将无形变成有形。]]></content>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2018%2F05%2F29%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[0、概述0.1、回归的含义 高尔顿（Frramcia Galton,1882-1911）早年在剑桥大学学习医学，但医生的职业对 他并无吸引力，后来他接受了一笔遗产，这使他可以放弃医生的生涯，并与 1850－1852年期 间去非洲考察，他所取得的成就使其在1853年获得英国皇家地理学会的金质奖章。此后他研究 过多种学科（气象学、心理学、社会学、 教育学和指纹学等），在1865年后他的主要兴趣转 向遗传学，这也许是受他表兄达尔文的影响。 从19世纪80年代高尔顿就开始思考父代和子代相似，如身高、性格及其它种种特制的相似 性问题。于是他选择了父母平均身高X与其一子身高Y的关系作为研究对象。他观察了1074对 父母及每对父母的一个儿子，将结果描成散点图，发现趋势近乎一条直 线。总的来说是父母 平均身高X增加时，其子的身高Y也倾向于增加，这是意料中的结果。但有意思的是高尔顿发现 这1074对父母平均身高的平均值为68 英寸（英国计量单位，1 英寸=2.54cm）时，1074个 儿子的平均身高为69 英寸，比父母平均身高大1 英寸 ，于是他推想，当父母平均身高为64 英寸时，1074个儿子的平均身高应为64+1=65 英寸；若父母的身高为72 英寸时，他们儿子 的平均身高应为72=1=73 英寸，但观察结果确与此不符。高尔顿发现前一种情况是儿子的平 均身高为67 英寸，高于父母平均值达3 英寸，后者儿子的平均身高为71英寸，比父母的平均 身高低1 英寸。 高尔顿对此研究后得出的解释是自然界有一种约束力，使人类身高在一定时期是相对稳定 的。如果父 母身高（或矮了），其子女比他们更高（矮），则人类身材将向高、矮两个极端 分化。自然界不这样做，它让身高有一种回归到中心的作用。例如，父母平均身高 72 英寸， 这超过了平均值68英寸，表明这些父母属于高的一类，其儿子也倾向属于高的一类（其平均身 高71 英寸 大于子代69 英寸），但不像父母离子代那么远（71-69&lt;72-68）。反之，父母 平均身高64 英寸，属于矮的一类，其儿子也倾向属于矮的一类（其平均67 英寸，小于子代 的平均数69 英寸），但不像父母离中心那么远（69 -67&lt; 68-64）。 因此，身高有回归于中心的趋势，由于这个性质，高尔顿就把“回归”这个词引进到问题 的讨论中，这就是“回归”名称的由来，逐渐背后人沿用成习了。 0.2、线性回归线性回归实际上就是找到一条直线 $$y = W^{T}x + b$$ ，使得该直线尽可能的拟合样本数据。 0.3、Logistic回归Logistic回归其实不是线性回归求预测值的问题，而是二分类问题。首先我们的线性回归模型输出的预测值，连续的数值，我们想用它解决分类问题，就需要让连续的数值转换到0/1就可以了,这里引入一个新的函数sigmoid $$y=\frac{1}{1+e^{-z}}$$ 函数，其中 $$z = W^{T}x + b$$ 。图像是这样的： 0.4、Softmax回归In mathematics, the softmax function, or normalized exponential function,[1]:198 is ageneralization of the logistic function that “squashes” a K-dimensional vector $$\mathbf {z}$$ ofarbitrary real values to a K-dimensional vector $$\sigma (\mathbf {z} )$$ of real values, where eachentry is in the range (0, 1), and all the entries adds up to 1. The function is given by$${\displaystyle \sigma :\mathbb {R} ^{K}\to \left\{\sigma \in \mathbb {R} ^{K}|\sigma _{i}&gt;0,\sum _{i=1}^{K}\sigma _{i}=1\right\}}$$$$\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} for j = 1, …, K.$$举个栗子：假设模型的输入样本是I，讨论一个3分类问题（类别用1，2，3表示），样本I的真实类别是2，那么这个样本I经过网络所有层到达 softmax 层之前就得到了 $$W^{T}x$$，也就是说 $$W^{T}x$$ 是一个3 1的向量，那么上面公式中的 $$a_{j}$$ 就表示这个3 1的向量中的第 j 个值; 而分母中的 $$a_{k}$$ 则表示3 1的向量中的3个值，所以会有个求和符号。 因为 $$e^{x}$$ 恒大于0，所以分子永远是正数，分母又是多个正数的和，所以分母也肯定是正数，因此 $$\sigma (\mathbf {z} )_{j}$$ 是正数，而且范围是(0,1)。如果现在不是在训练模型，而是在测试模型，那么当一个样本经过 softmax 层并输出一个K 1的向量时，就会取这个向量中值最大的那个数的index作为这个样本的预测标签。 总结一下：sigmoid将一个real value映射到（0,1）的区间（当然也可以是（-1,1）），这样可以用来做二分类。softmax把一个k维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中bi是一个0-1的常数，然后可以根据bi的大小来进行多分类的任务，如取权重最大的一维。 1、介绍1.1、工作原理1.2、优点，缺点，适用范围优点：计算代价不高，易于理解和实现。 缺点：容易欠拟合，分类精度可能不高。 适用范围：标称型数据，标称型数据。 1.3、一般流程收集数据：可以使用任何方法。 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。 分析数据：采用任意方法对数据进行分析。 训练算法：大部分时间用于训练，训练的目的是为了找到最佳的分类回归系数。 测试算法：一旦训练步骤完成，分类将会很快。 使用算法：首先，我们需要输入一些数据，并将其装换成对应的结构化数字；接着基于训练好的回归系数就可以对这些数据进行简单的回归计算，判定它们属于哪个类别；在此之后，我们就可以在输出的类别上做一些其他分析工作。 2、实现2.1、梯度下降梯度：对于可微的数量场f(x,y,z)，以 $$\left ( \partial f /\partial x, \partial f /\partial y, \partial f /\partial z\right )$$ 为分量的向量场称为f的梯度或斜量。梯度下降法(gradient descent)是一个最优化算法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。 对于只含有一组数据的训练样本，我们可以得到更新weights的规则为：$$\theta _{j} := \theta _{j} + \alpha ( y^{i} - h_{\theta }(x^{i}))x_{j}^{(i)}$$扩展到多组数据样本，更新公式为：Repeat until convergence {$$\theta _{j} := \theta _{j} + \alpha \sum_{i=1}^{m} ( y^{i} - h_{\theta }(x^{i}))x_{j}^{(i)} (for every j)$$}称为批处理梯度下降算法，这种更新算法所需要的运算成本很高，尤其是数据量较大时。考虑下面的更新算法：Loop { for i=1 to m,{$$\theta _{j} := \theta _{j} + \alpha ( y^{i} - h_{\theta }(x^{i}))x_{j}^{(i)} (for every j)$$​ }}该算法又叫做随机梯度下降法，这种算法不停的更新weights，每次使用一个样本数据进行更新。当数据量较大时，一般使用后者算法进行更新。 2.2、伪代码123456#随机梯度上升算法可以写成如下的伪代码：所有回归系数初始化为1对数据集中每个样本 计算该样本的梯度 使用alpha * gradient更新回归系数返回回归系数 2.3、python实现1234567891011121314151617181920212223242526272829303132333435363738# 梯度上升算法def gradAscent(dataMatIn,classLabels): ''' :param dataMatIn: 输入数据 :param classLabels: 每行数据对应的标签 :return: ''' dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() # m*n 矩阵 m,n = shape(dataMatrix) alpha = 0.001 maxCycles = 500 # n所有的Feature，都有weight weights = ones((n,1)) for k in range(maxCycles): h = sigmoid(dataMatrix*weights) # 计算实际值与预测值之间的差值 error = (labelMat - h) # 梯度上升，对sigmoid函数一阶偏导 weights = weights + alpha*dataMatrix.transpose()*error return weights# 随机梯度上升算法def stocGradAscent1(dataMatrix, classLabels, numIter=150): m,n = shape(dataMatrix) weights = ones(n) #initialize to all ones for j in range(numIter): dataIndex = list(range(m)) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration randIndex = int(random.uniform(0,len(dataIndex)))#Does not go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h # 梯度下降是预测值-实际值，h - y weights = weights + alpha * error * float64(dataMatrix[randIndex]) del(dataIndex[randIndex]) return weights 3、总结本算法接触到了机器学习的一个核心算法，梯度下降(上升)算法，改算法贯穿机器学习的各种算法，使用该算法可以快速求得最小偏差，得到我们建模的参数(w,b)。Logistic回归(逻辑回归)是个分类问题，使用微分知识对函数sigmoid $$y=\frac{1}{1+e^{-z}}$$ 求偏微分，不需要直接求导，而是用偏微分计算。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F05%2F06%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[0、概述朴素贝叶斯是贝叶斯理论的一部分，所以讲述这部分内容之前首先需要了解一下贝叶斯定理。 贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理。其中P(A|B)是在B发 生的情况下A发生的可能性。 描述两个条件概率之间的关系，比如 P(A|B) 和 P(B|A)。 按照乘法法则，可以立刻导出： P(A∩B) = P(A)*P(B|A) = P(B)*P(A|B) 如上公式也可变形为： P(B|A) = P(A|B)*P(B)/P(A) 朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立。 1、介绍1.1、工作原理假设每个特征对于决定样本属于哪一类都同等重要。给定一个测试样本w，计算它属于c的概 率，只需要计算出训练样本中类别c的每个特征类型的概率p(w|c)，所有训练样本中类别c的 概率p(c)，测试样本w的概率(实际使用中比较样本w属于哪个类别的可能性更大，等号两边相等) p(c|w) = p(w|c)p(c)/p(w) 1.2、优点，缺点，适用范围优点：在数据较少的情况下仍然有效，可以处理多类别问题。 缺点：对于输入数据的准备方式较为敏感。 适用范围：标称型数据。 1.3、一般流程收集数据：可以使用任何方法。 准备数据：需要数值型或者布尔型数据。 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。 训练算法：计算不同的独立特征的条件概率。 测试算法：计算错误率。 使用算法：常用于文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。 2、实现2.1、伪代码123456789计算每个类别中的文档数目对每篇训练文档： 对每个类别： 如果词条出现文档中 -&gt; 增加该词条的计数值 增加所有词条的计数值 对每个类别： 对每个词条： 将该词条的数目数除以总词条数目得到条件概率 返回每个类别的条件概率 2.2、python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet)def setOfWords2Vec(vocabList,inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print("the word: %s is no in my Vocabulary!"%word) return returnVecdef bagOfWords2VecMN(vocabList,inputSet): ''' 单个单词出现多次的场景 :param vocabList:单词集合 :param inputSet:输入的句子 :return: ''' returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 else: print("the word: %s is no in my Vocabulary!" % word) return returnVecdef trainNB0(trainMatrix,trainCategory): ''' :param trainMatrix:文档矩阵 :param trainCategory:文档标签向量 :return: ''' # 计算样本数 numTrainDocs = len(trainMatrix) # 计算单词数 numWords = len(trainMatrix[0]) # 计算所有样本中侮辱言论的概率 pAbusive = sum(trainCategory)/float(numTrainDocs) # 避免个别单词不出现，导致计算概率为0，默认都出现一次。 p0Num = ones(numWords);p1Num = ones(numWords) p0Denom = 2.0; p1Denom = 2.0 for i in range(numTrainDocs): # 轮询所有样本 if trainCategory[i] == 1: # 所有侮辱性言论 # 侮辱性言论中，每个单词使用的次数 p1Num += trainMatrix[i] # 侮辱性言论中，所有单词的出现次数 p1Denom += sum(trainMatrix[i]) else: # 所有正常言论 # 正常言论中，每个单词使用的次数 p0Num += trainMatrix[i] # 正常言论中，所有单词的出现次数 p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom) p0Vect = log(p0Num / p0Denom) return p0Vect,p1Vect,pAbusivedef classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): # p(c1|w) = p(w|c1)p(c1)/p(w) # p(c0|w) = p(w|c0)p(c0)/p(w) # p0Vec,p1Vec已经log过的。此处缺少除以单词本身出现的概率p(w)。p0,p1都需要除以p(w)可约去 p1 = sum(vec2Classify * p1Vec)+log(pClass1) p0 = sum(vec2Classify * p0Vec)+log(1 - pClass1) if p1 &gt; p0: return 1 else: return 0 3、总结巧妙的运用概率的方法解决分类问题，朴素贝叶斯降低了条件概率计算的复杂度，并且正确率很好。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树算法]]></title>
    <url>%2F2018%2F04%2F19%2F%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[0、概述分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。 结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 分类的时候，从根节点开始，对实例的某一个特征进行测试，根据测试结果，将实例分配到其子结点； 此时，每一个子结点对应着该特征的一个取值。如此递归向下移动，直至达到叶结点，最后将实例分配到叶结点的类中。 1、介绍1.1、工作原理对训练数据进行建模，决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。 它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 1.2、优点，缺点，适用范围优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过渡匹配问题。 适用范围：数值型和标称型 1.3、一般流程收集数据：可以使用任何方法。 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 训练算法：构造树的数据结构。 测试算法：使用经验树计算错误率。 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 2、实现2.1、伪代码123456789# 创建分支伪代码函数createBranch()如下所示： if so return 类标签; else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch()并增加结果到分支节点中 return 分支节点 2.2、信息增益划分数据集的大原则是：将无序的数据变得更加有序。决策树是通过什么来选择树根，树枝，叶子的呢？这里不得提到两个概念：信息增益和熵。信息增益：在划分数据集之前之后信息发生的变化称为信息增益。熵：集合信息的度量方式称为熵。如果待分类的事务可能分在多个分类之中，则符号x$_{i}$的信息定义为：$$l(x_{i}) = - \log_2 p(x_{i})$$其中p(x$_{i}$)是选择该分类的概率。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，公式如下：$$H = -\sum_{i=1}^{n}p(x_{i})\log_2 p(x_{i})$$ 2.3、python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118def calcShannoEnt(dataSet): ''' 计算给定数据集的香农熵。注意是所有数据的集合，不是单个样本。 1. get last current ShannoEnt :param dataSet:数据集 :return: 计算结果 ''' numEntries = len(dataSet) labelCounts = &#123;&#125; # 遍历每个实例，统计标签的频数 for featVec in dataSet: # 获取单个样本的标签列(样本的最后一列为标签列) currentLabel = featVec[-1] # 为单个分类创建字典 if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 # 计算所有类别所有可能值包含的信息期望值 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob*log(prob,2) return shannonEnt def splitDataSet(dataSet,axis,value): ''' 按照给定特征划分数据集。最终获得一个子集。举个栗子： [[1,1,'yes'] [1,1,'yes'] [1,0,'no'] [0,1,'no'] [0,1,'no']] 如果以第二例作为axis，特征值为1，得到下面的子集： [[1,'yes'] [1,'yes'] [0,'no'] [0,'no']] 1. find the value row 2. del the value from the row 3. return all the row as [] :param dataSet:待划分的数据集 :param axis:划分数据集的特征 :param value: 需要split的特征值 :return: 划分结果列表 ''' retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reduceFeatVec = featVec[:axis] reduceFeatVec.extend(featVec[axis+1:]) retDataSet.append(reduceFeatVec) return retDataSet def chooseBestFeatureToSplit(dataSet): ''' 1.每个特征中的每个特征值分类 2.计算划分后的香农熵，找出最大熵 3.得出划分后香农熵最大的特征作为最好的分类特征 :param dataSet:数据集 :return:返回分类最好的特征 ''' numFeatures = len(dataSet[0]) - 1 baseEntropy = calcShannoEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob*calcShannoEnt(subDataSet) infoGain = baseEntropy - newEntropy if(infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i return bestFeature def majorityCnt(classList): ''' 采用多数表决的方法决定叶结点的分类 :param: 所有的 类标签 列表 :return: 出现次数最多的类 ''' classCount=&#123;&#125; for vote in classList: # 统计所有类标签的频数 if vote not in classCount.keys(): classCount[vote]=0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] def createTree(dataSet,labels): ''' 创建决策树 :param: dataSet:训练数据集 :return: labels:所有的类标签 ''' classList = [example[-1] for example in dataSet] # 所有的标签都相同，直接返回该标签 if classList.count(classList[0]) == len(classList): return classList[0] # 遍历完所有特征，仍不能把数据划分为仅包含唯一类别的分组，返回出现次数最多的 if len(dataSet[0]) == 1: return majorityCnt(classList) # 找到最好的Feature，函数名称感觉有点问题，跟ToSplit没有关系 bestFeat = chooseBestFeatureToSplit(dataSet) # 找到对应的标签 bestFeatLabel = labels[bestFeat] # 将当前最好的Feature放到决策树 myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) # 一个Feature里面有多个值，按不同的值进行构建树的分支 for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTree 3、总结决策树算法稍微有点复杂，后面还会涉及到剪枝。目前只接触到二叉树，也就是说一个特征值 要么是，要么否，不知道支不支持多分支，看算应该是支持多分支的，有待继续学习。深刻感 觉到理论的重要性，香农熵的概念是这个算法的核心，如果没有选择最优特征值的算法，无法 确定最优二叉树。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2018%2F04%2F12%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[0、概述 1、介绍 1.1、工作原理 1.2、优点，缺点，适用范围 1.3、一般流程 2、实现 2.1、伪代码 2.2、python实现 3、总结 0、概述简单的说，k-近邻算法是一种分类算法，采用测量测试用例与样本用例不同特征值之间的距离，选取距离最小的样本所属的分类作为测试用例的分类。 1、介绍1.1、工作原理工作原理：存在一个样本数据集，也称作训练样本集，并且样本集中每个数据都存在标签，即我们都知道样本集中每个数据与所属分类的对应关系。 输入没有标签的新数据后，将新数据的每个特征与样本集中的数据对应的特征比较，然后算法提取样本集中特征最相似数据(最近邻)的分类标签。 一般来说，我们只选择样本数据集中前K个最相似的数据，这就是k-近邻算法中的k的出处，通常k是不大于20的整数。 最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类 1.2、优点，缺点，适用范围优点：精度高、对异常值不敏感、无数据输入假定。 缺点：计算复杂度高、空间复杂度高。（每个测试用例都要计算与所有样本用例的每个特征值的距离） 适用范围：数值型和标称型 1.3、一般流程收集数据：可以使用任何方法。 准备数据：距离计算所需要的数值，最好是结构化的数据格式。 分析数据：可以使用任何方法。 训练算法：N/A 测试算法：计算错误率。 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类， 最后应用对计算出的分类执行后续的处理。 2、实现2.1、伪代码N/A 2.2、python实现12345678910111213141516171819202122232425262728293031323334from numpy import *import operatorfrom os import listdirdef classify0(inX, dataSet, labels, k): ''' kNN: k Nearest Neighbors Input: inX: vector to compare to existing dataset (1xN) dataSet: size m data set of known vectors (NxM) labels: data set labels (1xM vector) k: number of neighbors to use for comparison (should be an odd number) Output: the most popular class label ''' # 获取样本数，即M dataSetSize = dataSet.shape[0] # 将单个样本拉成M行矩阵，矩阵相减，得到测试用例特征值与每个样本用例的特征值的差 diffMat = tile(inX, (dataSetSize,1)) - dataSet # 特征值差的平方。此处是array sqDiffMat = diffMat**2 # 每个样本所有特征值差和 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 # 排序，按照数组所在的编号输出 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): # 获取前k个的标签 voteIlabel = labels[sortedDistIndicies[i]] # 计算相同标签数 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 # 按照标签个数排序 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 3、总结k-近邻算法是分类数据最简单最有效的算法。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习15之Animation动画]]></title>
    <url>%2F2017%2F12%2F28%2Fmatplotlib%E5%AD%A6%E4%B9%A015%E4%B9%8BAnimation%E5%8A%A8%E7%94%BB%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npfrom matplotlib import animationfig,ax = plt.subplots()# 数据是一个0~2π内的正弦曲线x = np.arange(0,2*np.pi,0.1)y= np.sin(x)line, = ax.plot(x,y)# 采用np.pi*i/30 方式更新，更流畅def animate(i): line.set_ydata(np.sin(x+np.pi*i/30)) return line,def init(): line.set_ydata(np.sin(x)) return line,# https://matplotlib.org/api/animation_api.htmlani = animation.FuncAnimation(fig=fig,func=animate,frames=30,init_func=init,blit=True)# https://stackoverflow.com/questions/25140952/matplotlib-save-animation-in-gif-errorani.save('/home/xuleilx/workspace/github/github_pages/public/images/animation.gif', writer='imagemagick', fps=30)plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习14之次坐标轴]]></title>
    <url>%2F2017%2F12%2F28%2Fmatplotlib%E5%AD%A6%E4%B9%A014%E4%B9%8B%E6%AC%A1%E5%9D%90%E6%A0%87%E8%BD%B4%2F</url>
    <content type="text"><![CDATA[代码：12345678910111213141516171819202122# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npx = np.arange(0,10,0.1)y1 = 0.05*x**2y2 = -1*y1# 获取figure默认的坐标系 ax1# fig,ax1 = plt.subplots()ax1 = plt.subplot()# 对ax1调用twinx()方法，生成如同镜面效果后的ax2ax2 = ax1.twinx()ax1.plot(x,y1,'g-')ax2.plot(x,y2,'b--')ax1.set_xlabel('X data')ax1.set_ylabel('Y1',color='g')ax2.set_ylabel('Y2',color='b')plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/subaxes.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习13之图中图]]></title>
    <url>%2F2017%2F12%2F28%2Fmatplotlib%E5%AD%A6%E4%B9%A013%E4%B9%8B%E5%9B%BE%E4%B8%AD%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[代码：1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-import matplotlib.pyplot as pltfig = plt.figure()x = [1,2,3,4,5,6,7]y = [1,3,4,2,5,8,6]# 4个值都是占整个figure坐标系的百分比。在这里，假设figure的大小是10x10，# 那么大图就被包含在由(1, 1)开始，宽8，高8的坐标系内。left,bottom,width,height=0.1,0.1,0.8,0.8ax1 = fig.add_axes([left,bottom,width,height])ax1.plot(x,y,'r')ax1.set_xlabel('X')ax1.set_ylabel('Y')ax1.set_title('title')left,bottom,width,height=0.2,0.6,0.25,0.25ax2 = fig.add_axes([left,bottom,width,height])ax2.plot(y,x,'b--')ax2.set_xlabel('X')ax2.set_ylabel('Y')ax2.set_title('title inside 1')# 采用一种更简单方法，即直接往plt里添加新的坐标系left,bottom,width,height=0.6,0.2,0.25,0.25plt.axes([left,bottom,width,height])plt.plot(y[::-1],x,'g-.') # 注意对y进行了逆序处理plt.xlabel('X')plt.ylabel('Y')plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/plotinplot.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习12之Subplot多合一显示2]]></title>
    <url>%2F2017%2F12%2F28%2Fmatplotlib%E5%AD%A6%E4%B9%A012%E4%B9%8BSubplot%E5%A4%9A%E5%90%88%E4%B8%80%E6%98%BE%E7%A4%BA2%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport matplotlib.gridspec as gridspec# method 1:subplot2grid#############################plt.figure()ax1 = plt.subplot2grid((3,3),(0,0),colspan=3,rowspan=1)ax1.plot([1,2],[1,2])ax1.set_title('ax1_title')ax2 = plt.subplot2grid((3,3),(1,0),colspan=2)ax3 = plt.subplot2grid((3,3),(1,2),rowspan=2)ax4 = plt.subplot2grid((3,3),(2,0))ax5 = plt.subplot2grid((3,3),(2,1))# method 2:gridspec############################## plt.figure()# gs = gridspec.GridSpec(3,3)# ax1 = plt.subplot(gs[0,:])# ax2 = plt.subplot(gs[1,:2])# ax3 = plt.subplot(gs[1:,2])# ax4 = plt.subplot(gs[2,0])# ax5 = plt.subplot(gs[2,1])# method 3:easy to define structure############################## f,axes = plt.subplots(2,2,sharex=True)# axes[0,0].scatter([1,2],[1,2])plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/subplot_method1.png")plt.tight_layout()plt.show() 结果：Method1:Method2:Method3:]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习12之Subplot多合一显示]]></title>
    <url>%2F2017%2F12%2F28%2Fmatplotlib%E5%AD%A6%E4%B9%A012%E4%B9%8BSubplot%E5%A4%9A%E5%90%88%E4%B8%80%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[代码：12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import matplotlib.pyplot as pltfig = plt.figure()# 使用plt.subplot(2,1,1)将整个图像窗口分为2行1列, 当前位置为1.# 使用plt.plot([0,1],[0,1])在第1个位置创建一个小图.plt.subplot(211)plt.plot([0,1],[0,1])# 使用plt.subplot(2,3,4)将整个图像窗口分为2行3列, 当前位置为4.# 使用plt.plot([0,1],[0,2])在第4个位置创建一个小图.## 这里需要解释一下为什么第4个位置放第2个小图. 上一步中使用plt.subplot(2,1,1)将整个图像窗口分为2行1列,# 第1个小图占用了第1个位置, 也就是整个第1行. 这一步中使用plt.subplot(2,3,4)将整个图像窗口分为2行3列,# 于是整个图像窗口的第1行就变成了3列, 也就是成了3个位置, 于是第2行的第1个位置是整个图像窗口的第4个位置.plt.subplot(234)plt.plot([0,1],[0,2])plt.subplot(235)plt.plot([0,1],[0,3])plt.subplot(236)plt.plot([0,1],[0,4])plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/subplot.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习11之3D图像]]></title>
    <url>%2F2017%2F12%2F25%2Fmatplotlib%E5%AD%A6%E4%B9%A011%E4%B9%8B3D%E5%9B%BE%E5%83%8F%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)# X,Y valuex = np.arange(-4,4,0.25)y = np.arange(-4,4,0.25)X,Y = np.meshgrid(x,y)# x-y 平面的网格R = np.sqrt(X**2+Y**2)# height valueZ = np.sin(R)# rstride 和 cstride 分别代表 row 和 column 的跨度ax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap='rainbow',edgecolor='black')# 添加 XY 平面的等高线ax.contourf(X,Y,Z,zdir='z',offset=-2,cmap='rainbow')ax.set_zlim(-2,2)plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/3d_data.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习9之Image图片]]></title>
    <url>%2F2017%2F12%2F25%2Fmatplotlib%E5%AD%A6%E4%B9%A010%E4%B9%8BImage%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npa = np.linspace(0,1,9).reshape(3,3)# 三行三列的格子，a代表每一个值，图像右边有一个注释，白色代表值最大的地方，颜色越深值越小。plt.imshow(a,interpolation='nearest',cmap='bone',origin='lower')# 添加一个colorbar ，其中我们添加一个shrink参数，使colorbar的长度变短为原来的92%plt.colorbar(shrink=.92)plt.xticks(())plt.yticks(())plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/image.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习9Contours等高线图]]></title>
    <url>%2F2017%2F12%2F25%2Fmatplotlib%E5%AD%A6%E4%B9%A09%E4%B9%8BContours%E7%AD%89%E9%AB%98%E7%BA%BF%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[代码：1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npdef f(x,y): # the height function return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2)n = 256x = np.linspace(-3,3,n)y = np.linspace(-3,3,n)# 编织成栅格X,Y = np.meshgrid(x,y)# use plt.contourf to filling contours# X,Y and value for (X,Y) pointplt.contourf(X,Y,f(X,Y),8,alpha=.75,cmap=plt.cm.hot)C = plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=.5)# 添加label，隐藏坐标轴plt.clabel(C,inline=True,fontsize=10)plt.xticks(())plt.yticks(())plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/contours.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习8之bar柱状图]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A08%E4%B9%8Bbar%E6%9F%B1%E7%8A%B6%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[代码：1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npn = 10X = np.arange(n)# 均匀分布Y1 = (1-X/float(n))*np.random.uniform(0.5,1,n)Y2 = (1-X/float(n))*np.random.uniform(0.5,1,n)plt.bar(X,+Y1,facecolor='#9999ff',edgecolor='white')plt.bar(X,-Y2,facecolor='#ff9999',edgecolor='white')for x,y in zip(X,Y1): # ha: horizontal alignment # va: vertical alignment plt.text(x,y,'%.2f'%y,ha='center',va='bottom')for x,y in zip(X,Y2): plt.text(x,-y,'%.2f'%y,ha='center',va='top')#设置显示范围plt.xlim(-.5,n)plt.ylim(-1.25,1.25)plt.xticks(())plt.yticks(())plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/bar_map.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习7之散点图]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A07%E4%B9%8B%E6%95%A3%E7%82%B9%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[代码：1234567891011121314151617181920# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npn = 1024# 高斯分布 http://blog.csdn.net/lanchunhui/article/details/50163669X = np.random.normal(0,1,n)Y = np.random.normal(0,1,n)T = np.arctan2(Y,X)# for color valueplt.scatter(X,Y,s=75,c=T,alpha=0.5)#设置显示范围plt.xlim(-1.5,1.5)plt.ylim(-1.5,1.5)# 去坐标plt.xticks(())plt.yticks(())plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/dot_map.png")plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习6之tick能见度]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A06%E4%B9%8Btick%E8%83%BD%E8%A7%81%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[代码：1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y = 0.1*xplt.figure(num=1,figsize=(8,5))# 单纯直线plt.plot(x,y,linewidth=10,zorder=1)plt.ylim(-2,2)# gca = 'get current axis'# 移动坐标轴位置ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position("bottom")ax.yaxis.set_ticks_position("left")ax.spines['bottom'].set_position(('data',0))ax.spines['left'].set_position(('data',0))# 当坐标轴的数字被遮挡时，调整线的透明度for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.7, zorder=2))plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/tick.png")#显示plt.show()]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习5之标注annotate]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A05%E4%B9%8B%E6%A0%87%E6%B3%A8annotate%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y = 2*x + 1plt.figure(num=1,figsize=(8,5))# 单纯直线plt.plot(x,y)# gca = 'get current axis'# 移动坐标轴位置ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position("bottom")ax.yaxis.set_ticks_position("left")ax.spines['bottom'].set_position(('data',0))ax.spines['left'].set_position(('data',0))x0 = 1y0 = 2*x0+1# x:[x0,x0],y:[0,y0] 矩阵运算plt.plot([x0,x0],[0,y0],'k--',linewidth=2.5)# set dot stylesplt.scatter([x0,],[y0,],s=50,color='b')# 添加标注# method1plt.annotate(r'$2x+1=%s$'%y0, xy=(x0,y0), xycoords='data', xytext=(+30,-30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3,rad=.2'))# method2plt.text(-1,3, r'$\mu\ \sigma_i\ \alpha_t$', fontdict=&#123;'size':16,'color':'r'&#125;)plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/annotation.png")#显示plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习4之legend]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A04%E4%B9%8Blegend%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y1 = x + 1#绘制曲线y2 = x**2plt.figure()# 设置坐标轴plt.xlim(-1,2)plt.ylim(-2,3)plt.xlabel('I am x')plt.ylabel('I am y')new_ticks= np.linspace(-1,2,5)# 设置坐标的粒度plt.xticks(new_ticks)# 用文字代替对应的数值plt.yticks([-2,-1,0,1,3], ['very bad','bad','normal','good','very good'])# 单纯直线l1, = plt.plot(x,y2,label='up')# 指定线的颜色, 宽度和类型l2, = plt.plot(x,y1,color='red',linewidth=5.0,linestyle='--',label='bottom')plt.legend(handles=[l1,l2],labels=['aaa','bbb'],loc='best')plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/legend.png")#显示plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习3之axis2]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A03%E4%B9%8Baxis2%2F</url>
    <content type="text"><![CDATA[代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y1 = x + 1#绘制曲线y2 = x**2plt.figure()# 单纯直线plt.plot(x,y2)# 指定线的颜色, 宽度和类型plt.plot(x,y1,color='red',linewidth=5.0,linestyle='--')# 设置坐标轴plt.xlim(-1,2)plt.ylim(-2,3)plt.xlabel('I am x')plt.ylabel('I am y')new_ticks= np.linspace(-1,2,5)# 设置坐标的粒度plt.xticks(new_ticks)# 用文字代替对应的数值plt.yticks([-2,-1,0,1,3], ['very bad','bad','normal','good','very good'])# gca = 'get current axis'ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position("bottom")ax.yaxis.set_ticks_position("left")ax.spines['bottom'].set_position(('data',0))ax.spines['left'].set_position(('data',0))plt.savefig("/home/xuleilx/workspace/github/github_pages/public/images/axis_2.png")#显示plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习3之axis]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A03%E4%B9%8Baxis%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324252627282930313233343536# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y1 = x + 1#绘制曲线y2 = x**2plt.figure()# 单纯直线plt.plot(x,y2)# 指定线的颜色, 宽度和类型plt.plot(x,y1,color='red',linewidth=5.0,linestyle='--')# 设置坐标轴plt.xlim(-1,2)plt.ylim(-2,3)plt.xlabel('I am x')plt.ylabel('I am y')new_ticks= np.linspace(-1,2,5)# 设置坐标的粒度plt.xticks(new_ticks)# 用文字代替对应的数值plt.yticks([-2,-1,0,1,3], ['very bad','bad','normal','good','very good'])#显示plt.show() 输出结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习2之figure]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A02%E4%B9%8Bfigure%2F</url>
    <content type="text"><![CDATA[代码：123456789101112131415161718192021222324# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-3,3,50)#绘制直线y1 = x + 1#绘制曲线y2 = x**2# figure 3，指定figure的编号并指定figure的大小plt.figure(num=3,figsize=(8,5))plt.plot(x,y1)plt.figure(&quot;f2&quot;)plt.plot(x,y2)# 指定线的颜色, 宽度和类型plt.plot(x,y1,color=&apos;red&apos;,linewidth=5.0,linestyle=&apos;--&apos;)#显示plt.show() 结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib学习1之直线曲线]]></title>
    <url>%2F2017%2F12%2F24%2Fmatplotlib%E5%AD%A6%E4%B9%A01%E4%B9%8B%E7%9B%B4%E7%BA%BF%E6%9B%B2%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[代码： 1234567891011121314151617# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as np# 产生数据x = np.linspace(-1,1,50)#绘制直线y = x + 1plt.plot(x,y)#绘制曲线y = x**2plt.plot(x,y)#显示plt.show() 显示结果：]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习之二次曲线]]></title>
    <url>%2F2017%2F12%2F17%2Ftensorflow%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%8C%E6%AC%A1%E6%9B%B2%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#coding:utf-8import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt# 构造添加神经层的函数def add_layer(inputs,in_size,out_size,activation_function=None): # 权重，in_size*out_size随机变量矩阵 Weights = tf.Variable(tf.random_normal([in_size,out_size])) # 偏差，1*out_size数组。 # 在机器学习中，biases的推荐值不为0，这里是在0向量的基础上+0.1 biases = tf.Variable(tf.zeros([1,out_size])+0.1) # y = Weights*x+biases 。 tf.matmul()是矩阵的乘法 Wx_plus_b = tf.matmul(inputs,Weights)+biases # 如果没有定义激励函数就是线性函数 if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# 生成x的值,类似# [[-1. ]# [-0.5]# [ 0. ]# [ 0.5]# [ 1. ]]x_data = np.linspace(-1,1,300)[:,np.newaxis]# 加了一个noise,这样看起来会更像真实情况.类型与x_data一样noise = np.random.normal(0,0.05,x_data.shape)# y = x**2 - 0.5 + noisey_data = np.square(x_data)-0.5+noise# 利用占位符定义我们所需的神经网络的输入。# tf.placeholder()就是代表占位符，这里的None代表无论输入有多少都可以，# 因为输入只有一个特征，所以这里是1xs=tf.placeholder(tf.float32,[None,1])ys=tf.placeholder(tf.float32,[None,1])# inputLayer hideLayer outputLayer# 1 10 1# hideLayerl1 = add_layer(xs,1,10,activation_function=tf.nn.relu)# outputLayerpredition = add_layer(l1,10,1,activation_function=None)# 误差的平方和，再求平均loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-predition),reduction_indices=[1]))train_step=tf.train.GradientDescentOptimizer(0.1).minimize(loss)init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)# 绘图fig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data,y_data)plt.ion()plt.show()for i in range(1000): sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;) if i % 50 == 0: #print sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;) predition_value= sess.run(predition,feed_dict=&#123;xs:x_data&#125;) lines =ax.plot(x_data,predition_value,'r-',lw=5) plt.pause(0.5) ax.lines.remove(lines[0])# input() 学习的结果：]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F12%2F17%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
